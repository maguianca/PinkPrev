{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-20T19:16:43.944675Z",
     "start_time": "2025-05-20T19:14:22.145287300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úî Salvat: dataOUTbreast\\benign_cc\\20586908_6c613a14b80a8591_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\20587148_fd746d25eb40b3dc_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\20587174_fd746d25eb40b3dc_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\20587294_e634830794f5c1bd_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\20587320_e634830794f5c1bd_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\20587466_d571b5880ad2a016_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\20587544_d571b5880ad2a016_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\20587638_f4b2d377f43ba0bd_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\20587758_81cd83d2f4d78528_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\20587836_81cd83d2f4d78528_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\20587902_8dbbd4e51f549ff0_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\20588020_024ee3569b2605dc_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\20588164_8d0b9620c53c0268_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\20588190_8d0b9620c53c0268_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\20588334_493155e17143edef_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\20588458_bf1a6aaadb05e3df_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\22427682_d713ef5849f98b6c_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\22427864_bbd6a3a35438c11b_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\22579847_301f1776aebbf5d2_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\22580015_6200187f3f1ccc18_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\22580192_5530d5782fc89dd7_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\22580218_5530d5782fc89dd7_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\22580341_5eae9beae14d26fd_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\22580367_5eae9beae14d26fd_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\22580492_2a5b932da4ce5ca1_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\22580680_fe7d005dcbbfb46d_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\22613624_dcafa6ba6374ec07_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\22613796_45c7f44839fd9e68_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\22613918_f23fa352e7de3dc7_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\22613944_f23fa352e7de3dc7_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\22614097_6bd24a0a42c19ce1_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\22614353_d065adcb9905b973_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\22614499_2dec4948fbe6336d_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\22614522_2dec4948fbe6336d_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\22670124_e1f51192f7bf3f5f_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\22670301_98429c0bdf78c0c7_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\22670442_7e677f3d530e41ed_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\22670620_e15a16f87b4f9782_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\22670643_e15a16f87b4f9782_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\22670809_0b7396cdccacca82_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\22670832_0b7396cdccacca82_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\22670978_f571fd4e63c718e3_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\22678449_60995d51033e24b8_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\22678472_60995d51033e24b8_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\22678622_61b13c59bcba149e_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\22678646_61b13c59bcba149e_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\22678787_64a22c47765f0c5c_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\22678810_64a22c47765f0c5c_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\22678953_b9a4da5f2dae63a9_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\22678980_b9a4da5f2dae63a9_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\24055051_2f1104b3cda7f145_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\24055078_2f1104b3cda7f145_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\24055382_1e10aef17c9fe149_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\24055627_6f1aef40b3775182_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\24055779_f0f1a133837b5137_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\24055806_f0f1a133837b5137_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\24055931_839819f2eadaf325_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\24055958_839819f2eadaf325_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\24058660_9e8db9e34d5275ef_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\24058686_9e8db9e34d5275ef_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\24065308_c4b995eddb3c510c_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\24065434_83db89f57aea498a_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\24065611_d8205a09c8173f44_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\24065734_5291e1aee2bbf5df_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\24065887_c01f83a1eb283270_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\24065914_c01f83a1eb283270_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\26933772_f8bfddc28e8045c0_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\26933801_f8bfddc28e8045c0_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\27829161_fbb55bf7fff48540_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\30011484_349323117bf0fd93_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\30011647_6968748e66837bc7_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\30011798_4f20c1285d8f0b1f_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\30011824_4f20c1285d8f0b1f_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\50993426_5d85ecc9cf26b254_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\50993643_b03f1dd34eb3c55f_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\50993697_b03f1dd34eb3c55f_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\50993841_de5e8d61e501a71b_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\50993868_de5e8d61e501a71b_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\50993949_de5e8d61e501a71b_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\50993976_de5e8d61e501a71b_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\50994164_cc9e66c5b31baab8_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\50994191_cc9e66c5b31baab8_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\50994273_cc9e66c5b31baab8_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\50994381_cc9e66c5b31baab8_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\50994408_cc9e66c5b31baab8_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\50994562_de4c34099d6ef8de_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\50994616_de4c34099d6ef8de_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\50994706_069212ec65a94339_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\50994787_069212ec65a94339_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\50994868_069212ec65a94339_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\50995789_0c735e8768d276b4_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\50995899_c94d8a1ebd452afe_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\50995990_d742ec2f9b90aa62_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\50996110_71c1a60d57c5322f_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\50996137_71c1a60d57c5322f_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\50996228_8c1b2bd64ca4d778_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\50996379_6aba0b402889a16f_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\50996736_330e5fe16929eed4_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\50996854_fdf4a1516f88b280_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\50996881_fdf4a1516f88b280_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\50996999_ce5e5e18a261cd29_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\50997026_ce5e5e18a261cd29_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\50997053_ce5e5e18a261cd29_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\50997134_ce5e5e18a261cd29_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\50997277_9054942f7be52dd9_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\50997304_9054942f7be52dd9_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\50997488_97ec8cadfca70d32_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\50997515_97ec8cadfca70d32_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\50997651_67cc8c9939d74a9a_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\50997678_67cc8c9939d74a9a_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\50997796_cbb6c98a81e69eeb_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\50997823_cbb6c98a81e69eeb_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\50998032_66adfbb4f19c76d2_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\50998231_f34ee0ab6591b792_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\50998258_f34ee0ab6591b792_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\50998349_1e4b534393d18753_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\50998467_1f139436acfc5467_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\50998494_1f139436acfc5467_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\50998634_cd12bc20b3d27d0b_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\50998661_cd12bc20b3d27d0b_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\50999148_cb65e8dac169f596_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\50999175_cb65e8dac169f596_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\50999300_cb65e8dac169f596_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\50999327_cb65e8dac169f596_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\51048765_3f22cdda8da215e3_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\51048945_f3e93e889a7746f0_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\51048972_f3e93e889a7746f0_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\51049107_8c105bb715bf1c3c_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\51049134_8c105bb715bf1c3c_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\51049249_832ebce700241036_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\51049516_6f64793857feb5d0_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\51049543_6f64793857feb5d0_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\51049682_6f64793857feb5d0_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\51070197_6f64793857feb5d0_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\53580665_40e22f2e3215b954_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\53580692_40e22f2e3215b954_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\53580885_51bec6477a7898b9_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\53581033_4c341dad22471922_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\53581060_4c341dad22471922_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\53581151_3be876aecfaad4ca_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\53581264_80123a24997098dc_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\53581433_b231a8ba4dd4214f_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\53581796_573747ee33ef6e5a_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\53581914_21e6cc12630e5e9f_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\53581941_21e6cc12630e5e9f_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\53582331_8913a7e0cf3bd74e_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\53582449_3f0db31711fc9795_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\53582476_3f0db31711fc9795_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\53582567_3e73f1c0670cfb0a_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\53582791_465aa5ec1b59efc6_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\53582818_465aa5ec1b59efc6_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\53586415_dda3c6969a34ff8e_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\53586442_dda3c6969a34ff8e_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\53586778_e5f3f68b9ce31228_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\53586805_e5f3f68b9ce31228_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\53586896_6ac23356b912ee9b_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\53587014_809e3f43339f93c6_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\53587041_809e3f43339f93c6_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\53587131_7b71aa9928e6975e_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\53587481_d2befe622e188943_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\53587508_d2befe622e188943_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\53587599_11e6732579acf692_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\53587663_5fb370d4c1c71974_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_cc\\53587744_5fb370d4c1c71974_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\20586960_6c613a14b80a8591_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\20587200_fd746d25eb40b3dc_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\20587226_fd746d25eb40b3dc_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\20587346_e634830794f5c1bd_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\20587372_e634830794f5c1bd_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\20587492_d571b5880ad2a016_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\20587518_d571b5880ad2a016_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\20587690_f4b2d377f43ba0bd_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\20587784_81cd83d2f4d78528_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\20587810_81cd83d2f4d78528_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\20587928_8dbbd4e51f549ff0_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\20588072_024ee3569b2605dc_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\20588138_8d0b9620c53c0268_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\20588216_8d0b9620c53c0268_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\20588308_493155e17143edef_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\20588510_bf1a6aaadb05e3df_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\20588654_036aff49b8ac84f0_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\20588680_036aff49b8ac84f0_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\22427728_d713ef5849f98b6c_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\22579754_bbd6a3a35438c11b_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\22579893_301f1776aebbf5d2_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\22580068_6200187f3f1ccc18_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\22580244_5530d5782fc89dd7_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\22580270_5530d5782fc89dd7_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\22580393_5eae9beae14d26fd_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\22580419_5eae9beae14d26fd_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\22580548_2a5b932da4ce5ca1_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\22580732_fe7d005dcbbfb46d_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\22613676_dcafa6ba6374ec07_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\22613848_45c7f44839fd9e68_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\22613970_f23fa352e7de3dc7_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\22613996_f23fa352e7de3dc7_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\22614150_6bd24a0a42c19ce1_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\22614405_d065adcb9905b973_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\22614545_2dec4948fbe6336d_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\22614568_2dec4948fbe6336d_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\22670177_e1f51192f7bf3f5f_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\22670347_98429c0bdf78c0c7_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\22670488_7e677f3d530e41ed_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\22670673_e15a16f87b4f9782_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\22670703_e15a16f87b4f9782_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\22670878_0b7396cdccacca82_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\22671003_f571fd4e63c718e3_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\22678495_60995d51033e24b8_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\22678518_60995d51033e24b8_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\22678670_61b13c59bcba149e_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\22678694_61b13c59bcba149e_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\22678833_64a22c47765f0c5c_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\22678856_64a22c47765f0c5c_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\22679008_b9a4da5f2dae63a9_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\22679036_b9a4da5f2dae63a9_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\24054997_2f1104b3cda7f145_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\24055024_2f1104b3cda7f145_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\24055176_606e9b184978a350_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\24055328_1e10aef17c9fe149_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\24055573_6f1aef40b3775182_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\24055725_f0f1a133837b5137_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\24055752_f0f1a133837b5137_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\24055877_839819f2eadaf325_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\24055904_839819f2eadaf325_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\24058712_9e8db9e34d5275ef_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\24058738_9e8db9e34d5275ef_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\24065270_c4b995eddb3c510c_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\24065380_83db89f57aea498a_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\24065557_d8205a09c8173f44_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\24065680_5291e1aee2bbf5df_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\24065833_c01f83a1eb283270_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\24065860_c01f83a1eb283270_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\26933830_f8bfddc28e8045c0_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\26933859_f8bfddc28e8045c0_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\27829215_fbb55bf7fff48540_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\30011530_349323117bf0fd93_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\30011700_6968748e66837bc7_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\30011850_4f20c1285d8f0b1f_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\30318067_4f20c1285d8f0b1f_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\50993399_5d85ecc9cf26b254_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\50993616_b03f1dd34eb3c55f_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\50993670_b03f1dd34eb3c55f_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\50993787_de5e8d61e501a71b_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\50993814_de5e8d61e501a71b_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\50993895_de5e8d61e501a71b_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\50993922_de5e8d61e501a71b_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\50994110_cc9e66c5b31baab8_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\50994137_cc9e66c5b31baab8_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\50994327_cc9e66c5b31baab8_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\50994354_cc9e66c5b31baab8_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\50994535_de4c34099d6ef8de_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\50994589_de4c34099d6ef8de_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\50994733_069212ec65a94339_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\50994760_069212ec65a94339_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\50994814_069212ec65a94339_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\50995762_0c735e8768d276b4_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\50995872_c94d8a1ebd452afe_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\50995963_d742ec2f9b90aa62_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\50996056_71c1a60d57c5322f_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\50996083_71c1a60d57c5322f_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\50996201_8c1b2bd64ca4d778_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\50996325_6aba0b402889a16f_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\50996709_330e5fe16929eed4_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\50996800_fdf4a1516f88b280_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\50996827_fdf4a1516f88b280_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\50996945_ce5e5e18a261cd29_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\50996972_ce5e5e18a261cd29_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\50997080_ce5e5e18a261cd29_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\50997107_ce5e5e18a261cd29_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\50997223_9054942f7be52dd9_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\50997250_9054942f7be52dd9_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\50997434_97ec8cadfca70d32_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\50997461_97ec8cadfca70d32_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\50997597_67cc8c9939d74a9a_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\50997624_67cc8c9939d74a9a_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\50997742_cbb6c98a81e69eeb_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\50997769_cbb6c98a81e69eeb_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\50998086_66adfbb4f19c76d2_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\50998177_f34ee0ab6591b792_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\50998204_f34ee0ab6591b792_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\50998322_1e4b534393d18753_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\50998413_1f139436acfc5467_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\50998440_1f139436acfc5467_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\50998580_cd12bc20b3d27d0b_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\50998607_cd12bc20b3d27d0b_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\50999094_cb65e8dac169f596_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\50999121_cb65e8dac169f596_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\50999246_cb65e8dac169f596_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\50999273_cb65e8dac169f596_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\51048738_3f22cdda8da215e3_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\51048891_f3e93e889a7746f0_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\51048918_f3e93e889a7746f0_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\51049053_8c105bb715bf1c3c_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\51049080_8c105bb715bf1c3c_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\51049276_832ebce700241036_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\51049462_6f64793857feb5d0_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\51049489_6f64793857feb5d0_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\51049628_6f64793857feb5d0_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\51049655_6f64793857feb5d0_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\53580611_40e22f2e3215b954_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\53580638_40e22f2e3215b954_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\53580831_51bec6477a7898b9_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\53580979_4c341dad22471922_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\53581006_4c341dad22471922_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\53581124_3be876aecfaad4ca_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\53581237_80123a24997098dc_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\53581379_b231a8ba4dd4214f_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\53581769_573747ee33ef6e5a_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\53581860_21e6cc12630e5e9f_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\53581887_21e6cc12630e5e9f_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\53582304_8913a7e0cf3bd74e_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\53582395_3f0db31711fc9795_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\53582422_3f0db31711fc9795_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\53582540_3e73f1c0670cfb0a_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\53582710_465aa5ec1b59efc6_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\53582737_465aa5ec1b59efc6_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\53582764_465aa5ec1b59efc6_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\53586361_dda3c6969a34ff8e_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\53586388_dda3c6969a34ff8e_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\53586724_e5f3f68b9ce31228_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\53586751_e5f3f68b9ce31228_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\53586869_6ac23356b912ee9b_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\53586960_809e3f43339f93c6_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\53586987_809e3f43339f93c6_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\53587104_7b71aa9928e6975e_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\53587427_d2befe622e188943_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\53587454_d2befe622e188943_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\53587572_11e6732579acf692_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\53587690_5fb370d4c1c71974_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\benign_mlo\\53587717_5fb370d4c1c71974_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_cc\\20586934_6c613a14b80a8591_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_cc\\20587054_b6a4f750c6df4f90_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_cc\\20587612_f4b2d377f43ba0bd_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_cc\\20587994_024ee3569b2605dc_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_cc\\20588562_bf1a6aaadb05e3df_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_cc\\22427705_d713ef5849f98b6c_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_cc\\22427840_bbd6a3a35438c11b_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_cc\\22579870_301f1776aebbf5d2_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_cc\\22580038_6200187f3f1ccc18_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_cc\\22580520_2a5b932da4ce5ca1_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_cc\\22580654_fe7d005dcbbfb46d_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_cc\\22613650_dcafa6ba6374ec07_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_cc\\22613770_45c7f44839fd9e68_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_cc\\22614074_6bd24a0a42c19ce1_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_cc\\22614236_1e5c3af078f74b05_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_cc\\22614379_d065adcb9905b973_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_cc\\22670094_e1f51192f7bf3f5f_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_cc\\22670278_98429c0bdf78c0c7_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_cc\\22670465_7e677f3d530e41ed_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_cc\\24055203_606e9b184978a350_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_cc\\24055355_1e10aef17c9fe149_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_cc\\24055483_ac3185e18ffdc7b6_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_cc\\24055502_ac3185e18ffdc7b6_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_cc\\24055654_6f1aef40b3775182_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_cc\\24065289_c4b995eddb3c510c_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_cc\\24065461_83db89f57aea498a_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_cc\\24065584_d8205a09c8173f44_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_cc\\24065761_5291e1aee2bbf5df_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_cc\\27829134_fbb55bf7fff48540_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_cc\\30011507_349323117bf0fd93_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_cc\\30011674_6968748e66837bc7_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_cc\\50994895_069212ec65a94339_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_cc\\50996406_6aba0b402889a16f_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_cc\\50998113_66adfbb4f19c76d2_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_cc\\50999008_a78eba834ef6ee88_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_cc\\50999459_f62fbf38fb208316_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_cc\\53580858_51bec6477a7898b9_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_cc\\53581460_b231a8ba4dd4214f_MG_R_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_cc\\53582683_465aa5ec1b59efc6_MG_L_CC_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_mlo\\20586986_6c613a14b80a8591_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_mlo\\20587080_b6a4f750c6df4f90_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_mlo\\20587664_f4b2d377f43ba0bd_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_mlo\\20588046_024ee3569b2605dc_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_mlo\\20588536_bf1a6aaadb05e3df_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_mlo\\22427751_d713ef5849f98b6c_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_mlo\\22579730_bbd6a3a35438c11b_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_mlo\\22579916_301f1776aebbf5d2_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_mlo\\22580098_6200187f3f1ccc18_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_mlo\\22580576_2a5b932da4ce5ca1_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_mlo\\22580706_fe7d005dcbbfb46d_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_mlo\\22613702_dcafa6ba6374ec07_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_mlo\\22613822_45c7f44839fd9e68_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_mlo\\22614127_6bd24a0a42c19ce1_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_mlo\\22614266_1e5c3af078f74b05_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_mlo\\22614431_d065adcb9905b973_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_mlo\\22670147_e1f51192f7bf3f5f_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_mlo\\22670324_98429c0bdf78c0c7_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_mlo\\22670511_7e677f3d530e41ed_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_mlo\\22670855_0b7396cdccacca82_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_mlo\\24055149_606e9b184978a350_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_mlo\\24055274_1e10aef17c9fe149_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_mlo\\24055445_ac3185e18ffdc7b6_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_mlo\\24055464_ac3185e18ffdc7b6_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_mlo\\24055600_6f1aef40b3775182_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_mlo\\24065251_c4b995eddb3c510c_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_mlo\\24065407_83db89f57aea498a_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_mlo\\24065530_d8205a09c8173f44_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_mlo\\24065707_5291e1aee2bbf5df_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_mlo\\27829188_fbb55bf7fff48540_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_mlo\\30011553_349323117bf0fd93_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_mlo\\30011727_6968748e66837bc7_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_mlo\\50994841_069212ec65a94339_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_mlo\\50996352_6aba0b402889a16f_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_mlo\\50998059_66adfbb4f19c76d2_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_mlo\\50998981_a78eba834ef6ee88_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_mlo\\50999432_f62fbf38fb208316_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_mlo\\53580804_51bec6477a7898b9_MG_L_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_mlo\\53581406_b231a8ba4dd4214f_MG_R_ML_ANON.png\n",
      "‚úî Salvat: dataOUTbreast\\malign_mlo\\53582656_465aa5ec1b59efc6_MG_L_ML_ANON.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pydicom\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Calea cƒÉtre datele DICOM (input) »ôi PNG (output)\n",
    "input_root = \"dataINbreast\"\n",
    "output_root = \"dataOUTbreast\"\n",
    "\n",
    "# Parcurge toate subdirectoarele\n",
    "for root, dirs, files in os.walk(input_root):\n",
    "    for file in files:\n",
    "        if file.lower().endswith(\".dcm\"):\n",
    "            input_path = os.path.join(root, file)\n",
    "            \n",
    "            # CreeazƒÉ calea de output corespunzƒÉtoare\n",
    "            relative_path = os.path.relpath(root, input_root)\n",
    "            output_dir = os.path.join(output_root, relative_path)\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "            # Define»ôte calea finalƒÉ pentru fi»ôierul PNG\n",
    "            output_file = os.path.splitext(file)[0] + \".png\"\n",
    "            output_path = os.path.join(output_dir, output_file)\n",
    "\n",
    "            try:\n",
    "                # Cite»ôte fi»ôierul DICOM\n",
    "                dicom = pydicom.dcmread(input_path)\n",
    "\n",
    "                # Ob»õine pixelii\n",
    "                pixel_array = dicom.pixel_array\n",
    "                pixel_array = pixel_array.astype(float)\n",
    "                pixel_array = (np.maximum(pixel_array, 0) / pixel_array.max()) * 255.0\n",
    "                pixel_array = pixel_array.astype(np.uint8)\n",
    "\n",
    "                # CreeazƒÉ »ôi salveazƒÉ imaginea\n",
    "                image = Image.fromarray(pixel_array)\n",
    "                image.save(output_path)\n",
    "\n",
    "                print(f\"‚úî Salvat: {output_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Eroare la {input_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úî Total imagini MLO: 206 (Benign: 0, Malign: 1)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Doar clasele dorite\n",
    "selected_classes = ['benign_mlo', 'malign_mlo']\n",
    "class_map = {'benign_mlo': 0, 'malign_mlo': 1}\n",
    "\n",
    "# TransformƒÉri\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "# Custom dataset doar pentru benign_mlo »ôi malign_mlo, cu etichete 0/1\n",
    "class MLODataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.samples = []\n",
    "        self.transform = transform\n",
    "        for cls in selected_classes:\n",
    "            cls_path = os.path.join(root_dir, cls)\n",
    "            label = class_map[cls]\n",
    "            for fname in os.listdir(cls_path):\n",
    "                if fname.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "                    self.samples.append((os.path.join(cls_path, fname), label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        image = Image.open(img_path).convert(\"L\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# CreeazƒÉ dataset-ul\n",
    "dataset = MLODataset(\"dataOUTbreast\", transform=transform)\n",
    "\n",
    "# Split train/val\n",
    "from torch.utils.data import random_split\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_data, val_data = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=16)\n",
    "\n",
    "print(f\"‚úî Total imagini MLO: {len(dataset)} (Benign: 0, Malign: 1)\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T19:49:37.822989900Z",
     "start_time": "2025-05-20T19:49:37.641671400Z"
    }
   },
   "id": "95e681c6f19b98e3",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5fb2c858f0ae483598be2da11b2f5a42"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\AI\\Incercare proiect AI\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\magui\\.cache\\huggingface\\hub\\models--google--vit-base-patch16-224-in21k. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "acef803dfee64c2aa7578698e2a43cae"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224-in21k\",\n",
    "    num_labels=2  # benign / malign\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T19:34:40.524173Z",
     "start_time": "2025-05-20T19:33:52.654985500Z"
    }
   },
   "id": "27a2968d736aeb77",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [03:36<00:00, 19.68s/it, Loss=0.0315, Acc=87.80%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 1: Loss = 0.0315, Accuracy = 87.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [04:24<00:00, 24.09s/it, Loss=0.0224, Acc=96.95%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 2: Loss = 0.0224, Accuracy = 96.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [03:39<00:00, 19.92s/it, Loss=0.0155, Acc=100.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 3: Loss = 0.0155, Accuracy = 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [03:48<00:00, 20.77s/it, Loss=0.0123, Acc=100.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 4: Loss = 0.0123, Accuracy = 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [03:36<00:00, 19.65s/it, Loss=0.0094, Acc=100.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 5: Loss = 0.0094, Accuracy = 100.00%\n",
      "‚úî Modelul a fost salvat √Æn: vit_mamografie_model_entropy_1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "class_weights = torch.tensor([1.0, 166 / 40], dtype=torch.float).to(device)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for images, labels in progress_bar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(pixel_values=images)\n",
    "        loss = loss_fn(outputs.logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.logits, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        progress_bar.set_postfix({\n",
    "            \"Loss\": f\"{running_loss / total:.4f}\",\n",
    "            \"Acc\": f\"{(correct / total) * 100:.2f}%\"\n",
    "        })\n",
    "\n",
    "    print(f\"‚úÖ Epoch {epoch+1}: Loss = {running_loss / total:.4f}, Accuracy = {(correct / total) * 100:.2f}%\")\n",
    "\n",
    "output_dir = \"vit_mamografie_model_entropy_1\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "model.save_pretrained(output_dir)\n",
    "print(f\"‚úî Modelul a fost salvat √Æn: {output_dir}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T20:38:14.178603500Z",
     "start_time": "2025-05-20T20:19:08.461855600Z"
    }
   },
   "id": "cfc2068a56ddc013",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Raport clasificare pe validare:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.78      0.55      0.64        33\n",
      "      Malign       0.17      0.38      0.23         8\n",
      "\n",
      "    accuracy                           0.51        41\n",
      "   macro avg       0.47      0.46      0.44        41\n",
      "weighted avg       0.66      0.51      0.56        41\n",
      "\n",
      "üßæ Confusion Matrix:\n",
      "[[18 15]\n",
      " [ 5  3]]\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "import torch\n",
    "\n",
    "# √éncarcƒÉ modelul salvat corect\n",
    "model = ViTForImageClassification.from_pretrained(\"vit_mamografie_model_entropy_1\")\n",
    "\n",
    "# Trimite pe GPU dacƒÉ existƒÉ\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "val_dataset = datasets.ImageFolder(\"validare_mlo\", transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(pixel_values=images)\n",
    "        _, preds = torch.max(outputs.logits, 1)\n",
    "\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "print(\"üìä Raport clasificare pe validare:\")\n",
    "print(classification_report(y_true, y_pred, target_names=[\"Benign\", \"Malign\"]))\n",
    "print(\"üßæ Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T20:39:44.655675400Z",
     "start_time": "2025-05-20T20:39:33.403855100Z"
    }
   },
   "id": "2bd55b340d9f658a",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(pixel_values=images)\n",
    "        preds = torch.argmax(outputs.logits, axis=1).cpu().numpy()\n",
    "        y_pred.extend(preds)\n",
    "        y_true.extend(labels.numpy())\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=dataset.classes))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cffe4e52ac501d07"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [03:01<00:00, 13.97s/it, Train_Loss=0.0442, Train_Acc=78.16%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 1: Train Loss = 0.0442, Accuracy = 78.16%\n",
      "üß™ Validation F1-score: 0.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.78      0.88      0.83        33\n",
      "      Malign       0.00      0.00      0.00         8\n",
      "\n",
      "    accuracy                           0.71        41\n",
      "   macro avg       0.39      0.44      0.41        41\n",
      "weighted avg       0.63      0.71      0.67        41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [02:38<00:00, 12.17s/it, Train_Loss=0.0439, Train_Acc=46.60%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 2: Train Loss = 0.0439, Accuracy = 46.60%\n",
      "üß™ Validation F1-score: 0.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.79      0.91      0.85        33\n",
      "      Malign       0.00      0.00      0.00         8\n",
      "\n",
      "    accuracy                           0.73        41\n",
      "   macro avg       0.39      0.45      0.42        41\n",
      "weighted avg       0.64      0.73      0.68        41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [02:39<00:00, 12.25s/it, Train_Loss=0.0412, Train_Acc=81.07%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 3: Train Loss = 0.0412, Accuracy = 81.07%\n",
      "üß™ Validation F1-score: 0.1818\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.82      0.94      0.87        33\n",
      "      Malign       0.33      0.12      0.18         8\n",
      "\n",
      "    accuracy                           0.78        41\n",
      "   macro avg       0.57      0.53      0.53        41\n",
      "weighted avg       0.72      0.78      0.74        41\n",
      "\n",
      "üíæ Model salvat (F1 = 0.1818) √Æn: vit_mamografie_best_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [02:37<00:00, 12.13s/it, Train_Loss=0.0387, Train_Acc=83.50%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 4: Train Loss = 0.0387, Accuracy = 83.50%\n",
      "üß™ Validation F1-score: 0.1667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.81      0.91      0.86        33\n",
      "      Malign       0.25      0.12      0.17         8\n",
      "\n",
      "    accuracy                           0.76        41\n",
      "   macro avg       0.53      0.52      0.51        41\n",
      "weighted avg       0.70      0.76      0.72        41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [02:35<00:00, 11.96s/it, Train_Loss=0.0348, Train_Acc=89.81%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 5: Train Loss = 0.0348, Accuracy = 89.81%\n",
      "üß™ Validation F1-score: 0.2667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.79      0.45      0.58        33\n",
      "      Malign       0.18      0.50      0.27         8\n",
      "\n",
      "    accuracy                           0.46        41\n",
      "   macro avg       0.49      0.48      0.42        41\n",
      "weighted avg       0.67      0.46      0.52        41\n",
      "\n",
      "üíæ Model salvat (F1 = 0.2667) √Æn: vit_mamografie_best_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10:  31%|‚ñà‚ñà‚ñà       | 4/13 [00:54<02:01, 13.54s/it, Train_Loss=0.0270, Train_Acc=95.31%]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim import AdamW\n",
    "from transformers import ViTForImageClassification\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============================\n",
    "# üîß CONFIG\n",
    "# ============================\n",
    "train_dir = \"databreast\"\n",
    "val_dir = \"validare_mlo\"\n",
    "batch_size = 16\n",
    "num_epochs = 10\n",
    "early_stop_patience = 3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ============================\n",
    "# üì¶ TRANSFORMƒÇRI\n",
    "# ============================\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "# ============================\n",
    "# üì• √éNCƒÇRCARE DATE\n",
    "# ============================\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=transform_train)\n",
    "val_dataset = datasets.ImageFolder(val_dir, transform=transform_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# ============================\n",
    "# üß† MODEL\n",
    "# ============================\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224-in21k\",\n",
    "    num_labels=2  # benign / malign\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# ============================\n",
    "# ‚öñÔ∏è LOSS + OPTIMIZER\n",
    "# ============================\n",
    "# Pondere pentru clasa minoritarƒÉ (malign = 40)\n",
    "class_weights = torch.tensor([1.0, 166 / 40], dtype=torch.float).to(device)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# ============================\n",
    "# üö¶ TRAIN + VALIDARE + EARLY STOPPING\n",
    "# ============================\n",
    "best_f1 = 0\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for images, labels in progress_bar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(pixel_values=images)\n",
    "        loss = loss_fn(outputs.logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.logits, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        progress_bar.set_postfix({\n",
    "            \"Train_Loss\": f\"{running_loss / total:.4f}\",\n",
    "            \"Train_Acc\": f\"{(correct / total) * 100:.2f}%\"\n",
    "        })\n",
    "\n",
    "    print(f\"‚úÖ Epoch {epoch+1}: Train Loss = {running_loss / total:.4f}, Accuracy = {(correct / total) * 100:.2f}%\")\n",
    "\n",
    "    # VALIDARE\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(pixel_values=images)\n",
    "            _, preds = torch.max(outputs.logits, 1)\n",
    "\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    print(f\"üß™ Validation F1-score: {f1:.4f}\")\n",
    "    print(classification_report(y_true, y_pred, target_names=[\"Benign\", \"Malign\"]))\n",
    "\n",
    "    # EARLY STOPPING\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        epochs_no_improve = 0\n",
    "        output_dir = \"vit_mamografie_best_model\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        model.save_pretrained(output_dir)\n",
    "        print(f\"üíæ Model salvat (F1 = {f1:.4f}) √Æn: {output_dir}\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f\"‚èπÔ∏è Early stopping activat. F1 maxim: {best_f1:.4f}\")\n",
    "            break\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2025-05-20T20:51:39.603089900Z"
    }
   },
   "id": "873ca4998302a144",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "PRIMA EPOCA DOAR MALIGNE"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b1caae874ead04a"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'benign_mlo': 0, 'malign_mlo': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [06:14<00:00, 13.86s/it, Train_Loss=0.0331, Train_Acc=51.18%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 1: Train Loss = 0.0331, Accuracy = 51.18%\n",
      "üß™ Validation F1-score: 0.2903\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.00      0.00      0.00        88\n",
      "      Malign       0.17      1.00      0.29        18\n",
      "\n",
      "    accuracy                           0.17       106\n",
      "   macro avg       0.08      0.50      0.15       106\n",
      "weighted avg       0.03      0.17      0.05       106\n",
      "\n",
      "üìÑ Raport salvat √Æn: classification_report_epoch1.txt\n",
      "üñºÔ∏è Matrice de confuzie salvatƒÉ √Æn: conf_matrix_epoch1.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\AI\\Incercare proiect AI\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\AI\\Incercare proiect AI\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\AI\\Incercare proiect AI\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Model salvat √Æn: vit_mamografie_5_24_2025_model11_f10.2903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 13/27 [11:39<42:13, 180.94s/it, Train_Loss=0.0295, Train_Acc=49.52%]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim import AdamW\n",
    "from transformers import ViTForImageClassification\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============================\n",
    "# üîß CONFIG\n",
    "# ============================\n",
    "train_dir = \"model\"       # con»õine benign_mlo »ôi malign_mlo\n",
    "val_dir = \"validare\"      # con»õine benign_mlo »ôi malign_mlo\n",
    "batch_size = 16\n",
    "num_epochs = 10\n",
    "early_stop_patience = 5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ============================\n",
    "# üì¶ TRANSFORMƒÇRI\n",
    "# ============================\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "    transforms.RandomAffine(degrees=5, translate=(0.05, 0.05), scale=(0.95, 1.05)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5] * 3, [0.5] * 3)\n",
    "])\n",
    "\n",
    "# ============================\n",
    "# üì• √éNCƒÇRCARE DATE + SAMPLER\n",
    "# ============================\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=transform_train)\n",
    "val_dataset = datasets.ImageFolder(val_dir, transform=transform_val)\n",
    "print(train_dataset.class_to_idx)\n",
    "# sampler pentru echilibrare clase\n",
    "targets = train_dataset.targets\n",
    "class_count = [targets.count(0), targets.count(1)]\n",
    "weights = [1.0 / c for c in class_count]\n",
    "samples_weight = [weights[t] for t in targets]\n",
    "sampler = WeightedRandomSampler(samples_weight, num_samples=len(samples_weight), replacement=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# ============================\n",
    "# üß† MODEL\n",
    "# ============================\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224-in21k\",\n",
    "    num_labels=2\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# ============================\n",
    "# ‚öñÔ∏è LOSS + OPTIMIZER\n",
    "# ============================\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=torch.tensor([1.0, class_count[0]/class_count[1]]).to(device))\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# ============================\n",
    "# üö¶ TRAIN + VALIDARE\n",
    "# ============================\n",
    "best_f1 = 0\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for images, labels in progress_bar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(pixel_values=images)\n",
    "        loss = loss_fn(outputs.logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.logits, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        progress_bar.set_postfix({\n",
    "            \"Train_Loss\": f\"{running_loss / total:.4f}\",\n",
    "            \"Train_Acc\": f\"{(correct / total) * 100:.2f}%\"\n",
    "        })\n",
    "\n",
    "    print(f\"‚úÖ Epoch {epoch+1}: Train Loss = {running_loss / total:.4f}, Accuracy = {(correct / total) * 100:.2f}%\")\n",
    "\n",
    "    # VALIDARE\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(pixel_values=images)\n",
    "            _, preds = torch.max(outputs.logits, 1)\n",
    "\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    print(f\"üß™ Validation F1-score: {f1:.4f}\")\n",
    "\n",
    "    # SalveazƒÉ raportul de clasificare\n",
    "    report = classification_report(y_true, y_pred, target_names=[\"Benign\", \"Malign\"])\n",
    "    print(report)\n",
    "    report_path = f\"classification_report_epoch{epoch+1}.txt\"\n",
    "    with open(report_path, \"w\") as f:\n",
    "        f.write(report)\n",
    "    print(f\"üìÑ Raport salvat √Æn: {report_path}\")\n",
    "\n",
    "    # Matrice de confuzie\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Benign\", \"Malign\"])\n",
    "    disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
    "    plt.title(f\"Confusion Matrix - Epoch {epoch+1}\")\n",
    "    plt.savefig(f\"conf_matrix_epoch{epoch+1}.png\")\n",
    "    plt.close()\n",
    "    print(f\"üñºÔ∏è Matrice de confuzie salvatƒÉ √Æn: conf_matrix_epoch{epoch+1}.png\")\n",
    "\n",
    "    # EARLY STOPPING\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        epochs_no_improve = 0\n",
    "        output_dir = f\"vit_mamografie_5_24_2025_model1{epoch+1}_f1{f1:.4f}\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        model.save_pretrained(output_dir)\n",
    "        print(f\"üíæ Model salvat √Æn: {output_dir}\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f\"‚èπÔ∏è Early stopping activat. F1 maxim: {best_f1:.4f}\")\n",
    "            break\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2025-05-24T10:55:44.791593700Z"
    }
   },
   "id": "635b5eceae7bbd53",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "FOARTE PROST MODEL ___________ASA NU"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe33c55cb7e8b543"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etichete clase: {'benign_mlo': 0, 'malign_mlo': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [05:19<00:00, 11.85s/it, Train_Loss=0.0442, Train_Acc=74.41%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 1: Train Loss = 0.0442, Accuracy = 74.41%\n",
      "üß™ Validation F1-score: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [07:07<00:00, 15.83s/it, Train_Loss=0.0423, Train_Acc=71.56%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 2: Train Loss = 0.0423, Accuracy = 71.56%\n",
      "üß™ Validation F1-score: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:   0%|          | 0/27 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 85\u001B[39m\n\u001B[32m     82\u001B[39m running_loss, correct, total = \u001B[32m0.0\u001B[39m, \u001B[32m0\u001B[39m, \u001B[32m0\u001B[39m\n\u001B[32m     84\u001B[39m progress_bar = tqdm(train_loader, desc=\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch+\u001B[32m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_epochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m85\u001B[39m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mimages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mprogress_bar\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m     86\u001B[39m \u001B[43m    \u001B[49m\u001B[43mimages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mimages\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     87\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutputs\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpixel_values\u001B[49m\u001B[43m=\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\AI\\Incercare proiect AI\\.venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001B[39m, in \u001B[36mtqdm.__iter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1178\u001B[39m time = \u001B[38;5;28mself\u001B[39m._time\n\u001B[32m   1180\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1181\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43miterable\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m   1182\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01myield\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\n\u001B[32m   1183\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Update and possibly print the progressbar.\u001B[39;49;00m\n\u001B[32m   1184\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;49;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\AI\\Incercare proiect AI\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001B[39m, in \u001B[36m_BaseDataLoaderIter.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    730\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    731\u001B[39m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[32m    732\u001B[39m     \u001B[38;5;28mself\u001B[39m._reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m733\u001B[39m data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    734\u001B[39m \u001B[38;5;28mself\u001B[39m._num_yielded += \u001B[32m1\u001B[39m\n\u001B[32m    735\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    736\u001B[39m     \u001B[38;5;28mself\u001B[39m._dataset_kind == _DatasetKind.Iterable\n\u001B[32m    737\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    738\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._num_yielded > \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called\n\u001B[32m    739\u001B[39m ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\AI\\Incercare proiect AI\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1491\u001B[39m, in \u001B[36m_MultiProcessingDataLoaderIter._next_data\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1488\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._process_data(data, worker_id)\n\u001B[32m   1490\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m._shutdown \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._tasks_outstanding > \u001B[32m0\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1491\u001B[39m idx, data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_get_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1492\u001B[39m \u001B[38;5;28mself\u001B[39m._tasks_outstanding -= \u001B[32m1\u001B[39m\n\u001B[32m   1493\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._dataset_kind == _DatasetKind.Iterable:\n\u001B[32m   1494\u001B[39m     \u001B[38;5;66;03m# Check for _IterableDatasetStopIteration\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\AI\\Incercare proiect AI\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1453\u001B[39m, in \u001B[36m_MultiProcessingDataLoaderIter._get_data\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1449\u001B[39m     \u001B[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001B[39;00m\n\u001B[32m   1450\u001B[39m     \u001B[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001B[39;00m\n\u001B[32m   1451\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1452\u001B[39m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1453\u001B[39m         success, data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_try_get_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1454\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m success:\n\u001B[32m   1455\u001B[39m             \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\AI\\Incercare proiect AI\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1284\u001B[39m, in \u001B[36m_MultiProcessingDataLoaderIter._try_get_data\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m   1271\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_try_get_data\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001B[32m   1272\u001B[39m     \u001B[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001B[39;00m\n\u001B[32m   1273\u001B[39m     \u001B[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1281\u001B[39m     \u001B[38;5;66;03m# Returns a 2-tuple:\u001B[39;00m\n\u001B[32m   1282\u001B[39m     \u001B[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001B[39;00m\n\u001B[32m   1283\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1284\u001B[39m         data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_data_queue\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1285\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mTrue\u001B[39;00m, data)\n\u001B[32m   1286\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m   1287\u001B[39m         \u001B[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001B[39;00m\n\u001B[32m   1288\u001B[39m         \u001B[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001B[39;00m\n\u001B[32m   1289\u001B[39m         \u001B[38;5;66;03m# worker failures.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\queues.py:113\u001B[39m, in \u001B[36mQueue.get\u001B[39m\u001B[34m(self, block, timeout)\u001B[39m\n\u001B[32m    111\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m block:\n\u001B[32m    112\u001B[39m     timeout = deadline - time.monotonic()\n\u001B[32m--> \u001B[39m\u001B[32m113\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_poll\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[32m    114\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m Empty\n\u001B[32m    115\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m._poll():\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\connection.py:256\u001B[39m, in \u001B[36m_ConnectionBase.poll\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m    254\u001B[39m \u001B[38;5;28mself\u001B[39m._check_closed()\n\u001B[32m    255\u001B[39m \u001B[38;5;28mself\u001B[39m._check_readable()\n\u001B[32m--> \u001B[39m\u001B[32m256\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_poll\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\connection.py:329\u001B[39m, in \u001B[36mPipeConnection._poll\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m    326\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._got_empty_message \u001B[38;5;129;01mor\u001B[39;00m\n\u001B[32m    327\u001B[39m             _winapi.PeekNamedPipe(\u001B[38;5;28mself\u001B[39m._handle)[\u001B[32m0\u001B[39m] != \u001B[32m0\u001B[39m):\n\u001B[32m    328\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m329\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mbool\u001B[39m(\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\connection.py:1066\u001B[39m, in \u001B[36mwait\u001B[39m\u001B[34m(object_list, timeout)\u001B[39m\n\u001B[32m   1063\u001B[39m                 ready_objects.add(o)\n\u001B[32m   1064\u001B[39m                 timeout = \u001B[32m0\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1066\u001B[39m     ready_handles = \u001B[43m_exhaustive_wait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwaithandle_to_obj\u001B[49m\u001B[43m.\u001B[49m\u001B[43mkeys\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1067\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m   1068\u001B[39m     \u001B[38;5;66;03m# request that overlapped reads stop\u001B[39;00m\n\u001B[32m   1069\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m ov \u001B[38;5;129;01min\u001B[39;00m ov_list:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\connection.py:998\u001B[39m, in \u001B[36m_exhaustive_wait\u001B[39m\u001B[34m(handles, timeout)\u001B[39m\n\u001B[32m    996\u001B[39m ready = []\n\u001B[32m    997\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m L:\n\u001B[32m--> \u001B[39m\u001B[32m998\u001B[39m     res = \u001B[43m_winapi\u001B[49m\u001B[43m.\u001B[49m\u001B[43mWaitForMultipleObjects\u001B[49m\u001B[43m(\u001B[49m\u001B[43mL\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    999\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m res == WAIT_TIMEOUT:\n\u001B[32m   1000\u001B[39m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim import AdamW\n",
    "from transformers import ViTForImageClassification\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============================\n",
    "# üîß CONFIG\n",
    "# ============================\n",
    "train_dir = \"model\"       # con»õine benign_mlo »ôi malign_mlo\n",
    "val_dir = \"validare\"      # con»õine benign_mlo »ôi malign_mlo\n",
    "batch_size = 16\n",
    "num_epochs = 10\n",
    "early_stop_patience = 5\n",
    "min_epochs = 3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ============================\n",
    "# üì¶ TRANSFORMƒÇRI\n",
    "# ============================\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5] * 3, [0.5] * 3)\n",
    "])\n",
    "\n",
    "# ============================\n",
    "# üì• √éNCƒÇRCARE DATE\n",
    "# ============================\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=transform_train)\n",
    "val_dataset = datasets.ImageFolder(val_dir, transform=transform_val)\n",
    "print(\"Etichete clase:\", train_dataset.class_to_idx)\n",
    "\n",
    "# distribu»õie clase pentru ponderare loss\n",
    "targets = train_dataset.targets\n",
    "class_count = [targets.count(0), targets.count(1)]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# ============================\n",
    "# üß† MODEL\n",
    "# ============================\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224-in21k\",\n",
    "    num_labels=2\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# ============================\n",
    "# ‚öñÔ∏è LOSS + OPTIMIZER\n",
    "# ============================\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=torch.tensor([1.0, class_count[0]/class_count[1]]).to(device))\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# ============================\n",
    "# üö¶ TRAIN + VALIDARE\n",
    "# ============================\n",
    "best_f1 = 0\n",
    "epochs_no_improve = 0\n",
    "best_model_state = None\n",
    "best_y_true = []\n",
    "best_y_pred = []\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for images, labels in progress_bar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(pixel_values=images)\n",
    "        loss = loss_fn(outputs.logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.logits, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        progress_bar.set_postfix({\n",
    "            \"Train_Loss\": f\"{running_loss / total:.4f}\",\n",
    "            \"Train_Acc\": f\"{(correct / total) * 100:.2f}%\"\n",
    "        })\n",
    "\n",
    "    print(f\"‚úÖ Epoch {epoch+1}: Train Loss = {running_loss / total:.4f}, Accuracy = {(correct / total) * 100:.2f}%\")\n",
    "\n",
    "    # VALIDARE\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(pixel_values=images)\n",
    "            _, preds = torch.max(outputs.logits, 1)\n",
    "\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    print(f\"üß™ Validation F1-score: {f1:.4f}\")\n",
    "\n",
    "    # SalveazƒÉ cel mai bun model √Æn memorie\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_y_true = y_true.copy()\n",
    "        best_y_pred = y_pred.copy()\n",
    "        best_model_state = model.state_dict()\n",
    "        best_epoch = epoch + 1\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epoch + 1 >= min_epochs and epochs_no_improve >= early_stop_patience:\n",
    "            print(f\"‚èπÔ∏è Early stopping activat. F1 maxim: {best_f1:.4f}\")\n",
    "            break\n",
    "\n",
    "# ============================\n",
    "# ‚úÖ SALVARE FINALƒÇ\n",
    "# ============================\n",
    "final_dir = f\"vit_mamografie_5_24_2025_model1_best_epoch{best_epoch}_f1{best_f1:.4f}\"\n",
    "os.makedirs(final_dir, exist_ok=True)\n",
    "\n",
    "model.load_state_dict(best_model_state)\n",
    "model.save_pretrained(final_dir)\n",
    "print(f\"üíæ Modelul cu performan»õa cea mai bunƒÉ a fost salvat √Æn: {final_dir}\")\n",
    "\n",
    "# Raport + Matrice Confuzie\n",
    "report = classification_report(best_y_true, best_y_pred, target_names=[\"Benign\", \"Malign\"], zero_division=0)\n",
    "with open(os.path.join(final_dir, \"classification_report.txt\"), \"w\") as f:\n",
    "    f.write(report)\n",
    "print(f\"üìÑ Raport final salvat √Æn: {final_dir}/classification_report.txt\")\n",
    "\n",
    "cm = confusion_matrix(best_y_true, best_y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Benign\", \"Malign\"])\n",
    "disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
    "plt.title(f\"Confusion Matrix - Best Epoch {best_epoch}\")\n",
    "plt.savefig(os.path.join(final_dir, \"conf_matrix.png\"))\n",
    "plt.close()\n",
    "print(f\"üñºÔ∏è Matrice de confuzie finalƒÉ salvatƒÉ √Æn: {final_dir}/conf_matrix.png\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-24T11:47:20.759819500Z",
     "start_time": "2025-05-24T11:33:42.491163Z"
    }
   },
   "id": "4229c512d0517793",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "class_weights = torch.tensor([1.0, 166 / 40], dtype=torch.float).to(device)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for images, labels in progress_bar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(pixel_values=images)\n",
    "        loss = loss_fn(outputs.logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.logits, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        progress_bar.set_postfix({\n",
    "            \"Loss\": f\"{running_loss / total:.4f}\",\n",
    "            \"Acc\": f\"{(correct / total) * 100:.2f}%\"\n",
    "        })\n",
    "\n",
    "    print(f\"‚úÖ Epoch {epoch+1}: Loss = {running_loss / total:.4f}, Accuracy = {(correct / total) * 100:.2f}%\")\n",
    "\n",
    "output_dir = \"vit_mamografie_model_24mai\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "model.save_pretrained(output_dir)\n",
    "print(f\"‚úî Modelul a fost salvat √Æn: {output_dir}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62fcf1ca9661ee5e"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etichete clase: {'benign_mlo': 0, 'malign_mlo': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [05:10<00:00, 11.52s/it, Train_Loss=0.0285, Train_Acc=73.46%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 1: Train Loss = 0.0285, Accuracy = 73.46%\n",
      "üß™ Validation F1-score: 0.2903\n",
      "üü® Predic»õii val: [0, 106]\n",
      "üü¶ Etichete reale: [88, 18]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [05:06<00:00, 11.34s/it, Train_Loss=0.0229, Train_Acc=74.17%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 2: Train Loss = 0.0229, Accuracy = 74.17%\n",
      "üß™ Validation F1-score: 0.2903\n",
      "üü® Predic»õii val: [0, 106]\n",
      "üü¶ Etichete reale: [88, 18]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [08:16<00:00, 18.39s/it, Train_Loss=0.0196, Train_Acc=73.93%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 3: Train Loss = 0.0196, Accuracy = 73.93%\n",
      "üß™ Validation F1-score: 0.2903\n",
      "üü® Predic»õii val: [0, 106]\n",
      "üü¶ Etichete reale: [88, 18]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:   0%|          | 0/27 [00:00<?, ?it/s]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x000002AE07B8B100>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\AI\\Incercare proiect AI\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1663, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"C:\\AI\\Incercare proiect AI\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1627, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"C:\\Users\\magui\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\magui\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\popen_spawn_win32.py\", line 109, in wait\n",
      "    res = _winapi.WaitForSingleObject(int(self._handle), msecs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt: \n",
      "Epoch 4/10:   0%|          | 0/27 [00:11<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 92\u001B[39m\n\u001B[32m     90\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m images, labels \u001B[38;5;129;01min\u001B[39;00m progress_bar:\n\u001B[32m     91\u001B[39m     images, labels = images.to(device), labels.to(device)\n\u001B[32m---> \u001B[39m\u001B[32m92\u001B[39m     outputs = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpixel_values\u001B[49m\u001B[43m=\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     93\u001B[39m     loss = loss_fn(outputs.logits, labels)\n\u001B[32m     95\u001B[39m     optimizer.zero_grad()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\AI\\Incercare proiect AI\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\AI\\Incercare proiect AI\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\AI\\Incercare proiect AI\\.venv\\Lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:761\u001B[39m, in \u001B[36mViTForImageClassification.forward\u001B[39m\u001B[34m(self, pixel_values, head_mask, labels, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001B[39m\n\u001B[32m    753\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33mr\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    754\u001B[39m \u001B[33;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001B[39;00m\n\u001B[32m    755\u001B[39m \u001B[33;03m    Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\u001B[39;00m\n\u001B[32m    756\u001B[39m \u001B[33;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001B[39;00m\n\u001B[32m    757\u001B[39m \u001B[33;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001B[39;00m\n\u001B[32m    758\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    759\u001B[39m return_dict = return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m.config.use_return_dict\n\u001B[32m--> \u001B[39m\u001B[32m761\u001B[39m outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mvit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    762\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpixel_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    763\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    764\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    765\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    766\u001B[39m \u001B[43m    \u001B[49m\u001B[43minterpolate_pos_encoding\u001B[49m\u001B[43m=\u001B[49m\u001B[43minterpolate_pos_encoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    767\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    768\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    770\u001B[39m sequence_output = outputs[\u001B[32m0\u001B[39m]\n\u001B[32m    772\u001B[39m logits = \u001B[38;5;28mself\u001B[39m.classifier(sequence_output[:, \u001B[32m0\u001B[39m, :])\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\AI\\Incercare proiect AI\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\AI\\Incercare proiect AI\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\AI\\Incercare proiect AI\\.venv\\Lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:554\u001B[39m, in \u001B[36mViTModel.forward\u001B[39m\u001B[34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001B[39m\n\u001B[32m    548\u001B[39m     pixel_values = pixel_values.to(expected_dtype)\n\u001B[32m    550\u001B[39m embedding_output = \u001B[38;5;28mself\u001B[39m.embeddings(\n\u001B[32m    551\u001B[39m     pixel_values, bool_masked_pos=bool_masked_pos, interpolate_pos_encoding=interpolate_pos_encoding\n\u001B[32m    552\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m554\u001B[39m encoder_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    555\u001B[39m \u001B[43m    \u001B[49m\u001B[43membedding_output\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    556\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    557\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    558\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    559\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    560\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    561\u001B[39m sequence_output = encoder_outputs[\u001B[32m0\u001B[39m]\n\u001B[32m    562\u001B[39m sequence_output = \u001B[38;5;28mself\u001B[39m.layernorm(sequence_output)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\AI\\Incercare proiect AI\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\AI\\Incercare proiect AI\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\AI\\Incercare proiect AI\\.venv\\Lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:423\u001B[39m, in \u001B[36mViTEncoder.forward\u001B[39m\u001B[34m(self, hidden_states, head_mask, output_attentions, output_hidden_states, return_dict)\u001B[39m\n\u001B[32m    416\u001B[39m     layer_outputs = \u001B[38;5;28mself\u001B[39m._gradient_checkpointing_func(\n\u001B[32m    417\u001B[39m         layer_module.\u001B[34m__call__\u001B[39m,\n\u001B[32m    418\u001B[39m         hidden_states,\n\u001B[32m    419\u001B[39m         layer_head_mask,\n\u001B[32m    420\u001B[39m         output_attentions,\n\u001B[32m    421\u001B[39m     )\n\u001B[32m    422\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m423\u001B[39m     layer_outputs = \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    425\u001B[39m hidden_states = layer_outputs[\u001B[32m0\u001B[39m]\n\u001B[32m    427\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m output_attentions:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\AI\\Incercare proiect AI\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\AI\\Incercare proiect AI\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\AI\\Incercare proiect AI\\.venv\\Lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:381\u001B[39m, in \u001B[36mViTLayer.forward\u001B[39m\u001B[34m(self, hidden_states, head_mask, output_attentions)\u001B[39m\n\u001B[32m    379\u001B[39m \u001B[38;5;66;03m# in ViT, layernorm is also applied after self-attention\u001B[39;00m\n\u001B[32m    380\u001B[39m layer_output = \u001B[38;5;28mself\u001B[39m.layernorm_after(hidden_states)\n\u001B[32m--> \u001B[39m\u001B[32m381\u001B[39m layer_output = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mintermediate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlayer_output\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    383\u001B[39m \u001B[38;5;66;03m# second residual connection is done here\u001B[39;00m\n\u001B[32m    384\u001B[39m layer_output = \u001B[38;5;28mself\u001B[39m.output(layer_output, hidden_states)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\AI\\Incercare proiect AI\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\AI\\Incercare proiect AI\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\AI\\Incercare proiect AI\\.venv\\Lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:328\u001B[39m, in \u001B[36mViTIntermediate.forward\u001B[39m\u001B[34m(self, hidden_states)\u001B[39m\n\u001B[32m    327\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, hidden_states: torch.Tensor) -> torch.Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m328\u001B[39m     hidden_states = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdense\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    329\u001B[39m     hidden_states = \u001B[38;5;28mself\u001B[39m.intermediate_act_fn(hidden_states)\n\u001B[32m    331\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m hidden_states\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\AI\\Incercare proiect AI\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\AI\\Incercare proiect AI\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\AI\\Incercare proiect AI\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001B[39m, in \u001B[36mLinear.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    124\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m125\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim import AdamW\n",
    "from transformers import ViTForImageClassification\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============================\n",
    "# üîß CONFIG\n",
    "# ============================\n",
    "train_dir = \"model\"       # con»õine benign_mlo »ôi malign_mlo\n",
    "val_dir = \"validare\"      # con»õine benign_mlo »ôi malign_mlo\n",
    "batch_size = 16\n",
    "num_epochs = 10\n",
    "early_stop_patience = 5\n",
    "min_epochs = 3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ============================\n",
    "# üì¶ TRANSFORMƒÇRI\n",
    "# ============================\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5] * 3, [0.5] * 3)\n",
    "])\n",
    "\n",
    "# ============================\n",
    "# üì• √éNCƒÇRCARE DATE + SAMPLER\n",
    "# ============================\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=transform_train)\n",
    "val_dataset = datasets.ImageFolder(val_dir, transform=transform_val)\n",
    "\n",
    "print(\"Etichete clase:\", train_dataset.class_to_idx)  # {'benign_mlo': 0, 'malign_mlo': 1}\n",
    "\n",
    "targets = train_dataset.targets\n",
    "class_counts = [targets.count(0), targets.count(1)]\n",
    "\n",
    "# GreutƒÉ»õi ajustate (mai bl√¢nde dec√¢t inversul frecven»õei)\n",
    "weights = [1.0 / class_counts[0], 2.5 / class_counts[1]]\n",
    "samples_weight = [weights[t] for t in targets]\n",
    "sampler = WeightedRandomSampler(samples_weight, num_samples=len(samples_weight), replacement=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# ============================\n",
    "# üß† MODEL\n",
    "# ============================\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224-in21k\",\n",
    "    num_labels=2\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# ============================\n",
    "# ‚öñÔ∏è LOSS + OPTIMIZER\n",
    "# ============================\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=torch.tensor([1.0, 2.5]).to(device))\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# ============================\n",
    "# üö¶ TRAIN + VALIDARE\n",
    "# ============================\n",
    "best_f1 = 0\n",
    "epochs_no_improve = 0\n",
    "best_model_state = None\n",
    "best_y_true = []\n",
    "best_y_pred = []\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for images, labels in progress_bar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(pixel_values=images)\n",
    "        loss = loss_fn(outputs.logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.logits, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        progress_bar.set_postfix({\n",
    "            \"Train_Loss\": f\"{running_loss / total:.4f}\",\n",
    "            \"Train_Acc\": f\"{(correct / total) * 100:.2f}%\"\n",
    "        })\n",
    "\n",
    "    print(f\"‚úÖ Epoch {epoch+1}: Train Loss = {running_loss / total:.4f}, Accuracy = {(correct / total) * 100:.2f}%\")\n",
    "\n",
    "    # VALIDARE\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(pixel_values=images)\n",
    "            _, preds = torch.max(outputs.logits, 1)\n",
    "\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    print(f\"üß™ Validation F1-score: {f1:.4f}\")\n",
    "    print(\"üü® Predic»õii val:\", torch.tensor(y_pred).bincount().tolist())\n",
    "    print(\"üü¶ Etichete reale:\", torch.tensor(y_true).bincount().tolist())\n",
    "\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_y_true = y_true.copy()\n",
    "        best_y_pred = y_pred.copy()\n",
    "        best_model_state = model.state_dict()\n",
    "        best_epoch = epoch + 1\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epoch + 1 >= min_epochs and epochs_no_improve >= early_stop_patience:\n",
    "            print(f\"‚èπÔ∏è Early stopping activat. F1 maxim: {best_f1:.4f}\")\n",
    "            break\n",
    "\n",
    "# ============================\n",
    "# ‚úÖ SALVARE FINALƒÇ\n",
    "# ============================\n",
    "final_dir = f\"vit_mamografie_5_24_2025_model1_best_epoch{best_epoch}_f1{best_f1:.4f}\"\n",
    "os.makedirs(final_dir, exist_ok=True)\n",
    "\n",
    "model.load_state_dict(best_model_state)\n",
    "model.save_pretrained(final_dir)\n",
    "print(f\"üíæ Modelul cu performan»õa cea mai bunƒÉ a fost salvat √Æn: {final_dir}\")\n",
    "\n",
    "report = classification_report(best_y_true, best_y_pred, target_names=[\"Benign\", \"Malign\"], zero_division=0)\n",
    "with open(os.path.join(final_dir, \"classification_report.txt\"), \"w\") as f:\n",
    "    f.write(report)\n",
    "print(f\"üìÑ Raport final salvat √Æn: {final_dir}/classification_report.txt\")\n",
    "\n",
    "cm = confusion_matrix(best_y_true, best_y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Benign\", \"Malign\"])\n",
    "disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
    "plt.title(f\"Confusion Matrix - Best Epoch {best_epoch}\")\n",
    "plt.savefig(os.path.join(final_dir, \"conf_matrix.png\"))\n",
    "plt.close()\n",
    "print(f\"üñºÔ∏è Matrice de confuzie finalƒÉ salvatƒÉ √Æn: {final_dir}/conf_matrix.png\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-24T12:10:26.675299900Z",
     "start_time": "2025-05-24T11:49:53.422789800Z"
    }
   },
   "id": "c7385d680e9f9d91",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Convertit »ôi redimensionat: 20586960_6c613a14b80a8591_MG_R_ML_ANON.jpg ‚Üí 20586960_6c613a14b80a8591_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 20587200_fd746d25eb40b3dc_MG_R_ML_ANON.jpg ‚Üí 20587200_fd746d25eb40b3dc_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 20587226_fd746d25eb40b3dc_MG_L_ML_ANON.jpg ‚Üí 20587226_fd746d25eb40b3dc_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 20587346_e634830794f5c1bd_MG_R_ML_ANON.jpg ‚Üí 20587346_e634830794f5c1bd_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 20587372_e634830794f5c1bd_MG_L_ML_ANON.jpg ‚Üí 20587372_e634830794f5c1bd_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 20587492_d571b5880ad2a016_MG_R_ML_ANON.jpg ‚Üí 20587492_d571b5880ad2a016_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 20587518_d571b5880ad2a016_MG_L_ML_ANON.jpg ‚Üí 20587518_d571b5880ad2a016_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 20587690_f4b2d377f43ba0bd_MG_L_ML_ANON.jpg ‚Üí 20587690_f4b2d377f43ba0bd_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 20587784_81cd83d2f4d78528_MG_R_ML_ANON.jpg ‚Üí 20587784_81cd83d2f4d78528_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 20587810_81cd83d2f4d78528_MG_L_ML_ANON.jpg ‚Üí 20587810_81cd83d2f4d78528_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 20587928_8dbbd4e51f549ff0_MG_R_ML_ANON.jpg ‚Üí 20587928_8dbbd4e51f549ff0_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 20588072_024ee3569b2605dc_MG_L_ML_ANON.jpg ‚Üí 20588072_024ee3569b2605dc_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 20588138_8d0b9620c53c0268_MG_R_ML_ANON.jpg ‚Üí 20588138_8d0b9620c53c0268_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 20588216_8d0b9620c53c0268_MG_L_ML_ANON.jpg ‚Üí 20588216_8d0b9620c53c0268_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 20588308_493155e17143edef_MG_L_ML_ANON.jpg ‚Üí 20588308_493155e17143edef_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 20588510_bf1a6aaadb05e3df_MG_R_ML_ANON.jpg ‚Üí 20588510_bf1a6aaadb05e3df_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 20588654_036aff49b8ac84f0_MG_R_ML_ANON.jpg ‚Üí 20588654_036aff49b8ac84f0_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 20588680_036aff49b8ac84f0_MG_L_ML_ANON.jpg ‚Üí 20588680_036aff49b8ac84f0_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 22427728_d713ef5849f98b6c_MG_R_ML_ANON.jpg ‚Üí 22427728_d713ef5849f98b6c_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 22579754_bbd6a3a35438c11b_MG_L_ML_ANON.jpg ‚Üí 22579754_bbd6a3a35438c11b_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 22579893_301f1776aebbf5d2_MG_R_ML_ANON.jpg ‚Üí 22579893_301f1776aebbf5d2_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 22580068_6200187f3f1ccc18_MG_R_ML_ANON.jpg ‚Üí 22580068_6200187f3f1ccc18_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 22580244_5530d5782fc89dd7_MG_R_ML_ANON.jpg ‚Üí 22580244_5530d5782fc89dd7_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 22580270_5530d5782fc89dd7_MG_L_ML_ANON.jpg ‚Üí 22580270_5530d5782fc89dd7_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 22580393_5eae9beae14d26fd_MG_R_ML_ANON.jpg ‚Üí 22580393_5eae9beae14d26fd_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 22580419_5eae9beae14d26fd_MG_L_ML_ANON.jpg ‚Üí 22580419_5eae9beae14d26fd_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 22580548_2a5b932da4ce5ca1_MG_R_ML_ANON.jpg ‚Üí 22580548_2a5b932da4ce5ca1_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 22580732_fe7d005dcbbfb46d_MG_L_ML_ANON.jpg ‚Üí 22580732_fe7d005dcbbfb46d_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 22613676_dcafa6ba6374ec07_MG_R_ML_ANON.jpg ‚Üí 22613676_dcafa6ba6374ec07_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 22613848_45c7f44839fd9e68_MG_L_ML_ANON.jpg ‚Üí 22613848_45c7f44839fd9e68_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 22613970_f23fa352e7de3dc7_MG_R_ML_ANON.jpg ‚Üí 22613970_f23fa352e7de3dc7_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 22613996_f23fa352e7de3dc7_MG_L_ML_ANON.jpg ‚Üí 22613996_f23fa352e7de3dc7_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 22614150_6bd24a0a42c19ce1_MG_L_ML_ANON.jpg ‚Üí 22614150_6bd24a0a42c19ce1_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 22614405_d065adcb9905b973_MG_R_ML_ANON.jpg ‚Üí 22614405_d065adcb9905b973_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 22614545_2dec4948fbe6336d_MG_R_ML_ANON.jpg ‚Üí 22614545_2dec4948fbe6336d_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 22614568_2dec4948fbe6336d_MG_L_ML_ANON.jpg ‚Üí 22614568_2dec4948fbe6336d_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 22670177_e1f51192f7bf3f5f_MG_L_ML_ANON.jpg ‚Üí 22670177_e1f51192f7bf3f5f_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 22670347_98429c0bdf78c0c7_MG_L_ML_ANON.jpg ‚Üí 22670347_98429c0bdf78c0c7_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 22670488_7e677f3d530e41ed_MG_R_ML_ANON.jpg ‚Üí 22670488_7e677f3d530e41ed_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 22670673_e15a16f87b4f9782_MG_R_ML_ANON.jpg ‚Üí 22670673_e15a16f87b4f9782_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 22670703_e15a16f87b4f9782_MG_L_ML_ANON.jpg ‚Üí 22670703_e15a16f87b4f9782_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 22670878_0b7396cdccacca82_MG_L_ML_ANON.jpg ‚Üí 22670878_0b7396cdccacca82_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 22671003_f571fd4e63c718e3_MG_L_ML_ANON.jpg ‚Üí 22671003_f571fd4e63c718e3_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 22678495_60995d51033e24b8_MG_R_ML_ANON.jpg ‚Üí 22678495_60995d51033e24b8_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 22678518_60995d51033e24b8_MG_L_ML_ANON.jpg ‚Üí 22678518_60995d51033e24b8_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 22678670_61b13c59bcba149e_MG_R_ML_ANON.jpg ‚Üí 22678670_61b13c59bcba149e_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 22678694_61b13c59bcba149e_MG_L_ML_ANON.jpg ‚Üí 22678694_61b13c59bcba149e_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 22678833_64a22c47765f0c5c_MG_R_ML_ANON.jpg ‚Üí 22678833_64a22c47765f0c5c_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 22678856_64a22c47765f0c5c_MG_L_ML_ANON.jpg ‚Üí 22678856_64a22c47765f0c5c_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 22679008_b9a4da5f2dae63a9_MG_R_ML_ANON.jpg ‚Üí 22679008_b9a4da5f2dae63a9_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 22679036_b9a4da5f2dae63a9_MG_L_ML_ANON.jpg ‚Üí 22679036_b9a4da5f2dae63a9_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 24054997_2f1104b3cda7f145_MG_L_ML_ANON.jpg ‚Üí 24054997_2f1104b3cda7f145_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 24055024_2f1104b3cda7f145_MG_R_ML_ANON.jpg ‚Üí 24055024_2f1104b3cda7f145_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 24055176_606e9b184978a350_MG_R_ML_ANON.jpg ‚Üí 24055176_606e9b184978a350_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 24055328_1e10aef17c9fe149_MG_R_ML_ANON.jpg ‚Üí 24055328_1e10aef17c9fe149_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 24055573_6f1aef40b3775182_MG_L_ML_ANON.jpg ‚Üí 24055573_6f1aef40b3775182_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 24055725_f0f1a133837b5137_MG_L_ML_ANON.jpg ‚Üí 24055725_f0f1a133837b5137_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 24055752_f0f1a133837b5137_MG_R_ML_ANON.jpg ‚Üí 24055752_f0f1a133837b5137_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 24055877_839819f2eadaf325_MG_L_ML_ANON.jpg ‚Üí 24055877_839819f2eadaf325_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 24055904_839819f2eadaf325_MG_R_ML_ANON.jpg ‚Üí 24055904_839819f2eadaf325_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 24058712_9e8db9e34d5275ef_MG_R_ML_ANON.jpg ‚Üí 24058712_9e8db9e34d5275ef_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 24058738_9e8db9e34d5275ef_MG_L_ML_ANON.jpg ‚Üí 24058738_9e8db9e34d5275ef_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 24065270_c4b995eddb3c510c_MG_R_ML_ANON.jpg ‚Üí 24065270_c4b995eddb3c510c_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 24065380_83db89f57aea498a_MG_L_ML_ANON.jpg ‚Üí 24065380_83db89f57aea498a_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 24065557_d8205a09c8173f44_MG_R_ML_ANON.jpg ‚Üí 24065557_d8205a09c8173f44_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 24065680_5291e1aee2bbf5df_MG_L_ML_ANON.jpg ‚Üí 24065680_5291e1aee2bbf5df_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 24065833_c01f83a1eb283270_MG_L_ML_ANON.jpg ‚Üí 24065833_c01f83a1eb283270_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 24065860_c01f83a1eb283270_MG_R_ML_ANON.jpg ‚Üí 24065860_c01f83a1eb283270_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 26933830_f8bfddc28e8045c0_MG_R_ML_ANON.jpg ‚Üí 26933830_f8bfddc28e8045c0_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 26933859_f8bfddc28e8045c0_MG_L_ML_ANON.jpg ‚Üí 26933859_f8bfddc28e8045c0_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 27829215_fbb55bf7fff48540_MG_L_ML_ANON.jpg ‚Üí 27829215_fbb55bf7fff48540_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 30011530_349323117bf0fd93_MG_R_ML_ANON.jpg ‚Üí 30011530_349323117bf0fd93_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 30011700_6968748e66837bc7_MG_R_ML_ANON.jpg ‚Üí 30011700_6968748e66837bc7_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 30011850_4f20c1285d8f0b1f_MG_R_ML_ANON.jpg ‚Üí 30011850_4f20c1285d8f0b1f_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 30318067_4f20c1285d8f0b1f_MG_L_ML_ANON.jpg ‚Üí 30318067_4f20c1285d8f0b1f_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50993399_5d85ecc9cf26b254_MG_L_ML_ANON.jpg ‚Üí 50993399_5d85ecc9cf26b254_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50993616_b03f1dd34eb3c55f_MG_L_ML_ANON.jpg ‚Üí 50993616_b03f1dd34eb3c55f_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50993670_b03f1dd34eb3c55f_MG_L_ML_ANON.jpg ‚Üí 50993670_b03f1dd34eb3c55f_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50993787_de5e8d61e501a71b_MG_L_ML_ANON.jpg ‚Üí 50993787_de5e8d61e501a71b_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50993814_de5e8d61e501a71b_MG_R_ML_ANON.jpg ‚Üí 50993814_de5e8d61e501a71b_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50993895_de5e8d61e501a71b_MG_L_ML_ANON.jpg ‚Üí 50993895_de5e8d61e501a71b_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50993922_de5e8d61e501a71b_MG_R_ML_ANON.jpg ‚Üí 50993922_de5e8d61e501a71b_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50994110_cc9e66c5b31baab8_MG_L_ML_ANON.jpg ‚Üí 50994110_cc9e66c5b31baab8_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50994137_cc9e66c5b31baab8_MG_R_ML_ANON.jpg ‚Üí 50994137_cc9e66c5b31baab8_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50994327_cc9e66c5b31baab8_MG_L_ML_ANON.jpg ‚Üí 50994327_cc9e66c5b31baab8_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50994354_cc9e66c5b31baab8_MG_R_ML_ANON.jpg ‚Üí 50994354_cc9e66c5b31baab8_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50994535_de4c34099d6ef8de_MG_R_ML_ANON.jpg ‚Üí 50994535_de4c34099d6ef8de_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50994589_de4c34099d6ef8de_MG_R_ML_ANON.jpg ‚Üí 50994589_de4c34099d6ef8de_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50994733_069212ec65a94339_MG_L_ML_ANON.jpg ‚Üí 50994733_069212ec65a94339_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50994760_069212ec65a94339_MG_R_ML_ANON.jpg ‚Üí 50994760_069212ec65a94339_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50994814_069212ec65a94339_MG_L_ML_ANON.jpg ‚Üí 50994814_069212ec65a94339_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50995762_0c735e8768d276b4_MG_R_ML_ANON.jpg ‚Üí 50995762_0c735e8768d276b4_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50995872_c94d8a1ebd452afe_MG_L_ML_ANON.jpg ‚Üí 50995872_c94d8a1ebd452afe_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50995963_d742ec2f9b90aa62_MG_L_ML_ANON.jpg ‚Üí 50995963_d742ec2f9b90aa62_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50996056_71c1a60d57c5322f_MG_L_ML_ANON.jpg ‚Üí 50996056_71c1a60d57c5322f_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50996083_71c1a60d57c5322f_MG_R_ML_ANON.jpg ‚Üí 50996083_71c1a60d57c5322f_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50996201_8c1b2bd64ca4d778_MG_L_ML_ANON.jpg ‚Üí 50996201_8c1b2bd64ca4d778_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50996325_6aba0b402889a16f_MG_L_ML_ANON.jpg ‚Üí 50996325_6aba0b402889a16f_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50996709_330e5fe16929eed4_MG_R_ML_ANON.jpg ‚Üí 50996709_330e5fe16929eed4_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50996800_fdf4a1516f88b280_MG_L_ML_ANON.jpg ‚Üí 50996800_fdf4a1516f88b280_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50996827_fdf4a1516f88b280_MG_R_ML_ANON.jpg ‚Üí 50996827_fdf4a1516f88b280_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50996945_ce5e5e18a261cd29_MG_L_ML_ANON.jpg ‚Üí 50996945_ce5e5e18a261cd29_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50996972_ce5e5e18a261cd29_MG_R_ML_ANON.jpg ‚Üí 50996972_ce5e5e18a261cd29_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50997080_ce5e5e18a261cd29_MG_L_ML_ANON.jpg ‚Üí 50997080_ce5e5e18a261cd29_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50997107_ce5e5e18a261cd29_MG_R_ML_ANON.jpg ‚Üí 50997107_ce5e5e18a261cd29_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50997223_9054942f7be52dd9_MG_L_ML_ANON.jpg ‚Üí 50997223_9054942f7be52dd9_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50997250_9054942f7be52dd9_MG_R_ML_ANON.jpg ‚Üí 50997250_9054942f7be52dd9_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50997434_97ec8cadfca70d32_MG_L_ML_ANON.jpg ‚Üí 50997434_97ec8cadfca70d32_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50997461_97ec8cadfca70d32_MG_R_ML_ANON.jpg ‚Üí 50997461_97ec8cadfca70d32_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50997597_67cc8c9939d74a9a_MG_L_ML_ANON.jpg ‚Üí 50997597_67cc8c9939d74a9a_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50997624_67cc8c9939d74a9a_MG_R_ML_ANON.jpg ‚Üí 50997624_67cc8c9939d74a9a_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50997742_cbb6c98a81e69eeb_MG_L_ML_ANON.jpg ‚Üí 50997742_cbb6c98a81e69eeb_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50997769_cbb6c98a81e69eeb_MG_R_ML_ANON.jpg ‚Üí 50997769_cbb6c98a81e69eeb_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50998086_66adfbb4f19c76d2_MG_R_ML_ANON.jpg ‚Üí 50998086_66adfbb4f19c76d2_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50998177_f34ee0ab6591b792_MG_L_ML_ANON.jpg ‚Üí 50998177_f34ee0ab6591b792_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50998204_f34ee0ab6591b792_MG_R_ML_ANON.jpg ‚Üí 50998204_f34ee0ab6591b792_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50998322_1e4b534393d18753_MG_R_ML_ANON.jpg ‚Üí 50998322_1e4b534393d18753_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50998413_1f139436acfc5467_MG_L_ML_ANON.jpg ‚Üí 50998413_1f139436acfc5467_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50998440_1f139436acfc5467_MG_R_ML_ANON.jpg ‚Üí 50998440_1f139436acfc5467_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50998580_cd12bc20b3d27d0b_MG_L_ML_ANON.jpg ‚Üí 50998580_cd12bc20b3d27d0b_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50998607_cd12bc20b3d27d0b_MG_R_ML_ANON.jpg ‚Üí 50998607_cd12bc20b3d27d0b_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50999094_cb65e8dac169f596_MG_L_ML_ANON.jpg ‚Üí 50999094_cb65e8dac169f596_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50999121_cb65e8dac169f596_MG_R_ML_ANON.jpg ‚Üí 50999121_cb65e8dac169f596_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50999246_cb65e8dac169f596_MG_L_ML_ANON.jpg ‚Üí 50999246_cb65e8dac169f596_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50999273_cb65e8dac169f596_MG_R_ML_ANON.jpg ‚Üí 50999273_cb65e8dac169f596_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 51048738_3f22cdda8da215e3_MG_R_ML_ANON.jpg ‚Üí 51048738_3f22cdda8da215e3_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 51048891_f3e93e889a7746f0_MG_L_ML_ANON.jpg ‚Üí 51048891_f3e93e889a7746f0_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 51048918_f3e93e889a7746f0_MG_R_ML_ANON.jpg ‚Üí 51048918_f3e93e889a7746f0_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 51049053_8c105bb715bf1c3c_MG_L_ML_ANON.jpg ‚Üí 51049053_8c105bb715bf1c3c_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 51049080_8c105bb715bf1c3c_MG_R_ML_ANON.jpg ‚Üí 51049080_8c105bb715bf1c3c_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 51049276_832ebce700241036_MG_L_ML_ANON.jpg ‚Üí 51049276_832ebce700241036_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 51049462_6f64793857feb5d0_MG_L_ML_ANON.jpg ‚Üí 51049462_6f64793857feb5d0_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 51049489_6f64793857feb5d0_MG_R_ML_ANON.jpg ‚Üí 51049489_6f64793857feb5d0_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 51049628_6f64793857feb5d0_MG_L_ML_ANON.jpg ‚Üí 51049628_6f64793857feb5d0_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 51049655_6f64793857feb5d0_MG_R_ML_ANON.jpg ‚Üí 51049655_6f64793857feb5d0_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 53580611_40e22f2e3215b954_MG_L_ML_ANON.jpg ‚Üí 53580611_40e22f2e3215b954_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 53580638_40e22f2e3215b954_MG_R_ML_ANON.jpg ‚Üí 53580638_40e22f2e3215b954_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 53580831_51bec6477a7898b9_MG_R_ML_ANON.jpg ‚Üí 53580831_51bec6477a7898b9_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 53580979_4c341dad22471922_MG_L_ML_ANON.jpg ‚Üí 53580979_4c341dad22471922_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 53581006_4c341dad22471922_MG_R_ML_ANON.jpg ‚Üí 53581006_4c341dad22471922_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 53581124_3be876aecfaad4ca_MG_L_ML_ANON.jpg ‚Üí 53581124_3be876aecfaad4ca_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 53581237_80123a24997098dc_MG_R_ML_ANON.jpg ‚Üí 53581237_80123a24997098dc_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 53581379_b231a8ba4dd4214f_MG_L_ML_ANON.jpg ‚Üí 53581379_b231a8ba4dd4214f_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 53581769_573747ee33ef6e5a_MG_L_ML_ANON.jpg ‚Üí 53581769_573747ee33ef6e5a_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 53581860_21e6cc12630e5e9f_MG_L_ML_ANON.jpg ‚Üí 53581860_21e6cc12630e5e9f_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 53581887_21e6cc12630e5e9f_MG_R_ML_ANON.jpg ‚Üí 53581887_21e6cc12630e5e9f_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 53582304_8913a7e0cf3bd74e_MG_R_ML_ANON.jpg ‚Üí 53582304_8913a7e0cf3bd74e_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 53582395_3f0db31711fc9795_MG_L_ML_ANON.jpg ‚Üí 53582395_3f0db31711fc9795_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 53582422_3f0db31711fc9795_MG_R_ML_ANON.jpg ‚Üí 53582422_3f0db31711fc9795_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 53582540_3e73f1c0670cfb0a_MG_R_ML_ANON.jpg ‚Üí 53582540_3e73f1c0670cfb0a_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 53582710_465aa5ec1b59efc6_MG_R_ML_ANON.jpg ‚Üí 53582710_465aa5ec1b59efc6_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 53582737_465aa5ec1b59efc6_MG_L_ML_ANON.jpg ‚Üí 53582737_465aa5ec1b59efc6_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 53582764_465aa5ec1b59efc6_MG_R_ML_ANON.jpg ‚Üí 53582764_465aa5ec1b59efc6_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 53586361_dda3c6969a34ff8e_MG_L_ML_ANON.jpg ‚Üí 53586361_dda3c6969a34ff8e_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 53586388_dda3c6969a34ff8e_MG_R_ML_ANON.jpg ‚Üí 53586388_dda3c6969a34ff8e_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 53586724_e5f3f68b9ce31228_MG_L_ML_ANON.jpg ‚Üí 53586724_e5f3f68b9ce31228_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 53586751_e5f3f68b9ce31228_MG_R_ML_ANON.jpg ‚Üí 53586751_e5f3f68b9ce31228_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 53586869_6ac23356b912ee9b_MG_L_ML_ANON.jpg ‚Üí 53586869_6ac23356b912ee9b_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 53586960_809e3f43339f93c6_MG_L_ML_ANON.jpg ‚Üí 53586960_809e3f43339f93c6_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 53586987_809e3f43339f93c6_MG_R_ML_ANON.jpg ‚Üí 53586987_809e3f43339f93c6_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 53587104_7b71aa9928e6975e_MG_L_ML_ANON.jpg ‚Üí 53587104_7b71aa9928e6975e_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 53587427_d2befe622e188943_MG_L_ML_ANON.jpg ‚Üí 53587427_d2befe622e188943_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 53587454_d2befe622e188943_MG_R_ML_ANON.jpg ‚Üí 53587454_d2befe622e188943_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 53587572_11e6732579acf692_MG_L_ML_ANON.jpg ‚Üí 53587572_11e6732579acf692_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 53587690_5fb370d4c1c71974_MG_L_ML_ANON.jpg ‚Üí 53587690_5fb370d4c1c71974_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 53587717_5fb370d4c1c71974_MG_R_ML_ANON.jpg ‚Üí 53587717_5fb370d4c1c71974_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb001.jpg ‚Üí mdb001.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb002.jpg ‚Üí mdb002.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb003.jpg ‚Üí mdb003.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb004.jpg ‚Üí mdb004.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb005.jpg ‚Üí mdb005.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb006.jpg ‚Üí mdb006.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb007.jpg ‚Üí mdb007.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb008.jpg ‚Üí mdb008.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb009.jpg ‚Üí mdb009.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb010.jpg ‚Üí mdb010.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb011.jpg ‚Üí mdb011.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb012.jpg ‚Üí mdb012.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb013.jpg ‚Üí mdb013.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb014.jpg ‚Üí mdb014.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb015.jpg ‚Üí mdb015.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb016.jpg ‚Üí mdb016.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb017.jpg ‚Üí mdb017.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb018.jpg ‚Üí mdb018.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb019.jpg ‚Üí mdb019.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb020.jpg ‚Üí mdb020.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb021.jpg ‚Üí mdb021.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb022.jpg ‚Üí mdb022.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb024.jpg ‚Üí mdb024.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb025.jpg ‚Üí mdb025.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb026.jpg ‚Üí mdb026.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb027.jpg ‚Üí mdb027.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb029.jpg ‚Üí mdb029.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb030.jpg ‚Üí mdb030.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb031.jpg ‚Üí mdb031.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb032.jpg ‚Üí mdb032.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb033.jpg ‚Üí mdb033.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb034.jpg ‚Üí mdb034.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb035.jpg ‚Üí mdb035.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb036.jpg ‚Üí mdb036.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb037.jpg ‚Üí mdb037.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb038.jpg ‚Üí mdb038.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb039.jpg ‚Üí mdb039.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb040.jpg ‚Üí mdb040.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb041.jpg ‚Üí mdb041.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb042.jpg ‚Üí mdb042.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb043.jpg ‚Üí mdb043.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb044.jpg ‚Üí mdb044.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb045.jpg ‚Üí mdb045.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb046.jpg ‚Üí mdb046.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb047.jpg ‚Üí mdb047.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb048.jpg ‚Üí mdb048.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb049.jpg ‚Üí mdb049.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb050.jpg ‚Üí mdb050.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb051.jpg ‚Üí mdb051.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb052.jpg ‚Üí mdb052.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb053.jpg ‚Üí mdb053.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb054.jpg ‚Üí mdb054.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb055.jpg ‚Üí mdb055.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb056.jpg ‚Üí mdb056.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb057.jpg ‚Üí mdb057.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb059.jpg ‚Üí mdb059.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb060.jpg ‚Üí mdb060.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb061.jpg ‚Üí mdb061.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb062.jpg ‚Üí mdb062.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb063.jpg ‚Üí mdb063.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb064.jpg ‚Üí mdb064.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb065.jpg ‚Üí mdb065.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb066.jpg ‚Üí mdb066.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb067.jpg ‚Üí mdb067.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb068.jpg ‚Üí mdb068.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb069.jpg ‚Üí mdb069.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb070.jpg ‚Üí mdb070.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb071.jpg ‚Üí mdb071.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb073.jpg ‚Üí mdb073.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb074.jpg ‚Üí mdb074.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb076.jpg ‚Üí mdb076.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb077.jpg ‚Üí mdb077.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb078.jpg ‚Üí mdb078.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb079.jpg ‚Üí mdb079.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb080.jpg ‚Üí mdb080.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb081.jpg ‚Üí mdb081.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb082.jpg ‚Üí mdb082.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb083.jpg ‚Üí mdb083.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb084.jpg ‚Üí mdb084.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb085.jpg ‚Üí mdb085.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb086.jpg ‚Üí mdb086.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb087.jpg ‚Üí mdb087.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb088.jpg ‚Üí mdb088.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb089.jpg ‚Üí mdb089.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb091.jpg ‚Üí mdb091.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb093.jpg ‚Üí mdb093.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb094.jpg ‚Üí mdb094.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb096.jpg ‚Üí mdb096.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb097.jpg ‚Üí mdb097.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb098.jpg ‚Üí mdb098.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb099.jpg ‚Üí mdb099.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb100.jpg ‚Üí mdb100.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb101.jpg ‚Üí mdb101.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb103.jpg ‚Üí mdb103.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb104.jpg ‚Üí mdb104.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb106.jpg ‚Üí mdb106.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb107.jpg ‚Üí mdb107.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb108.jpg ‚Üí mdb108.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb109.jpg ‚Üí mdb109.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb112.jpg ‚Üí mdb112.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb113.jpg ‚Üí mdb113.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb114.jpg ‚Üí mdb114.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb116.jpg ‚Üí mdb116.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb118.jpg ‚Üí mdb118.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb119.jpg ‚Üí mdb119.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb121.jpg ‚Üí mdb121.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb122.jpg ‚Üí mdb122.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb123.jpg ‚Üí mdb123.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb126.jpg ‚Üí mdb126.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb127.jpg ‚Üí mdb127.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb128.jpg ‚Üí mdb128.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb129.jpg ‚Üí mdb129.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb131.jpg ‚Üí mdb131.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb132.jpg ‚Üí mdb132.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb133.jpg ‚Üí mdb133.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb135.jpg ‚Üí mdb135.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb136.jpg ‚Üí mdb136.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb137.jpg ‚Üí mdb137.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb138.jpg ‚Üí mdb138.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb139.jpg ‚Üí mdb139.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb140.jpg ‚Üí mdb140.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb142.jpg ‚Üí mdb142.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb143.jpg ‚Üí mdb143.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb145.jpg ‚Üí mdb145.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb146.jpg ‚Üí mdb146.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb147.jpg ‚Üí mdb147.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb149.jpg ‚Üí mdb149.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb150.jpg ‚Üí mdb150.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb151.jpg ‚Üí mdb151.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb152.jpg ‚Üí mdb152.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb153.jpg ‚Üí mdb153.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb154.jpg ‚Üí mdb154.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb156.jpg ‚Üí mdb156.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb157.jpg ‚Üí mdb157.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb159.jpg ‚Üí mdb159.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb160.jpg ‚Üí mdb160.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb161.jpg ‚Üí mdb161.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb162.jpg ‚Üí mdb162.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb163.jpg ‚Üí mdb163.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb164.jpg ‚Üí mdb164.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb165.jpg ‚Üí mdb165.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb166.jpg ‚Üí mdb166.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb167.jpg ‚Üí mdb167.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb168.jpg ‚Üí mdb168.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb169.jpg ‚Üí mdb169.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb172.jpg ‚Üí mdb172.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb173.jpg ‚Üí mdb173.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb174.jpg ‚Üí mdb174.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb175.jpg ‚Üí mdb175.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb176.jpg ‚Üí mdb176.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb177.jpg ‚Üí mdb177.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb180.jpg ‚Üí mdb180.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb182.jpg ‚Üí mdb182.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb183.jpg ‚Üí mdb183.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb185.jpg ‚Üí mdb185.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb187.jpg ‚Üí mdb187.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb188.jpg ‚Üí mdb188.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb189.jpg ‚Üí mdb189.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb190.jpg ‚Üí mdb190.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb191.jpg ‚Üí mdb191.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb192.jpg ‚Üí mdb192.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb193.jpg ‚Üí mdb193.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb194.jpg ‚Üí mdb194.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb195.jpg ‚Üí mdb195.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb196.jpg ‚Üí mdb196.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb197.jpg ‚Üí mdb197.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb198.jpg ‚Üí mdb198.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb199.jpg ‚Üí mdb199.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb200.jpg ‚Üí mdb200.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb201.jpg ‚Üí mdb201.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb203.jpg ‚Üí mdb203.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb204.jpg ‚Üí mdb204.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb205.jpg ‚Üí mdb205.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb207.jpg ‚Üí mdb207.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb208.jpg ‚Üí mdb208.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb210.jpg ‚Üí mdb210.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb212.jpg ‚Üí mdb212.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb214.jpg ‚Üí mdb214.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb215.jpg ‚Üí mdb215.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb217.jpg ‚Üí mdb217.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb218.jpg ‚Üí mdb218.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb219.jpg ‚Üí mdb219.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2013_BC010881_ MLO_L.jpg ‚Üí 2013_BC010881_ MLO_L.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2013_BC010881_ MLO_R.jpg ‚Üí 2013_BC010881_ MLO_R.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2013_BC011123_ MLO_L.jpg ‚Üí 2013_BC011123_ MLO_L.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2013_BC011123_ MLO_R.jpg ‚Üí 2013_BC011123_ MLO_R.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2016_BC009024_ MLO_R.jpg ‚Üí 2016_BC009024_ MLO_R.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2016_BC014181_ MLO_L.jpg ‚Üí 2016_BC014181_ MLO_L.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2016_BC016501_ MLO_L.jpg ‚Üí 2016_BC016501_ MLO_L.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2016_BC016501_ MLO_R.jpg ‚Üí 2016_BC016501_ MLO_R.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2016_BC016741_ MLO_L.jpg ‚Üí 2016_BC016741_ MLO_L.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2017_BC0020261_ MLO_R.jpg ‚Üí 2017_BC0020261_ MLO_R.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2017_BC0020361_ MLO_L.jpg ‚Üí 2017_BC0020361_ MLO_L.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2017_BC0020421_ MLO_R.jpg ‚Üí 2017_BC0020421_ MLO_R.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2017_BC0020521_ MLO_L.jpg ‚Üí 2017_BC0020521_ MLO_L.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2017_BC0020761_ MLO_L.jpg ‚Üí 2017_BC0020761_ MLO_L.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2017_BC0020761_ MLO_R.jpg ‚Üí 2017_BC0020761_ MLO_R.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2017_BC019561_ MLO_R.jpg ‚Üí 2017_BC019561_ MLO_R.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2017_BC019602_ MLO_L.jpg ‚Üí 2017_BC019602_ MLO_L.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2017_BC019721_ MLO_L.jpg ‚Üí 2017_BC019721_ MLO_L.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2017_BC019721_ MLO_R.jpg ‚Üí 2017_BC019721_ MLO_R.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2017_BC019744_ MLO_R.jpg ‚Üí 2017_BC019744_ MLO_R.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2017_BC019822_ MLO_L.jpg ‚Üí 2017_BC019822_ MLO_L.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2018_BC0021341_ MLO_L.jpg ‚Üí 2018_BC0021341_ MLO_L.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2018_BC0021341_ MLO_R.jpg ‚Üí 2018_BC0021341_ MLO_R.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2018_BC0021642_ MLO_L.jpg ‚Üí 2018_BC0021642_ MLO_L.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2018_BC0021723_ MLO_L.jpg ‚Üí 2018_BC0021723_ MLO_L.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2018_BC0021724_ MLO_L.jpg ‚Üí 2018_BC0021724_ MLO_L.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2018_BC0021724_ MLO_R.jpg ‚Üí 2018_BC0021724_ MLO_R.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2018_BC0021731_ MLO_L.jpg ‚Üí 2018_BC0021731_ MLO_L.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2018_BC0021735_ MLO_L.jpg ‚Üí 2018_BC0021735_ MLO_L.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2018_BC0021735_ MLO_R.jpg ‚Üí 2018_BC0021735_ MLO_R.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2018_BC0021746_ MLO_L.jpg ‚Üí 2018_BC0021746_ MLO_L.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2018_BC0021746_ MLO_R.jpg ‚Üí 2018_BC0021746_ MLO_R.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2018_BC0021865_ MLO_R.jpg ‚Üí 2018_BC0021865_ MLO_R.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2018_BC0021883_ MLO_L.jpg ‚Üí 2018_BC0021883_ MLO_L.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2018_BC0021981_ MLO_R.jpg ‚Üí 2018_BC0021981_ MLO_R.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2018_BC0022025_ MLO_L.jpg ‚Üí 2018_BC0022025_ MLO_L.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2018_BC0022025_ MLO_R.jpg ‚Üí 2018_BC0022025_ MLO_R.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2018_BC0022402_ MLO_L.jpg ‚Üí 2018_BC0022402_ MLO_L.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2018_BC0022402_ MLO_R.jpg ‚Üí 2018_BC0022402_ MLO_R.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2018_BC0022482_ MLO_R.jpg ‚Üí 2018_BC0022482_ MLO_R.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2018_BC0022603_ MLO_L.jpg ‚Üí 2018_BC0022603_ MLO_L.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2018_BC0022603_ MLO_R.jpg ‚Üí 2018_BC0022603_ MLO_R.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2018_BC005421_ MLO_L.jpg ‚Üí 2018_BC005421_ MLO_L.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2018_BC005421_ MLO_R.jpg ‚Üí 2018_BC005421_ MLO_R.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2018_BC011725_ MLO_L.jpg ‚Üí 2018_BC011725_ MLO_L.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2018_BC011725_ MLO_R.jpg ‚Üí 2018_BC011725_ MLO_R.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2018_BC017121_ MLO_R.jpg ‚Üí 2018_BC017121_ MLO_R.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2018_BC018161_ MLO_L.jpg ‚Üí 2018_BC018161_ MLO_L.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2018_BC018161_ MLO_R.jpg ‚Üí 2018_BC018161_ MLO_R.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 20586986_6c613a14b80a8591_MG_L_ML_ANON.jpg ‚Üí 20586986_6c613a14b80a8591_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 20587080_b6a4f750c6df4f90_MG_R_ML_ANON.jpg ‚Üí 20587080_b6a4f750c6df4f90_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 20587664_f4b2d377f43ba0bd_MG_R_ML_ANON.jpg ‚Üí 20587664_f4b2d377f43ba0bd_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 20588046_024ee3569b2605dc_MG_R_ML_ANON.jpg ‚Üí 20588046_024ee3569b2605dc_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 20588536_bf1a6aaadb05e3df_MG_L_ML_ANON.jpg ‚Üí 20588536_bf1a6aaadb05e3df_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 22427751_d713ef5849f98b6c_MG_L_ML_ANON.jpg ‚Üí 22427751_d713ef5849f98b6c_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 22579730_bbd6a3a35438c11b_MG_R_ML_ANON.jpg ‚Üí 22579730_bbd6a3a35438c11b_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 22579916_301f1776aebbf5d2_MG_L_ML_ANON.jpg ‚Üí 22579916_301f1776aebbf5d2_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 22580098_6200187f3f1ccc18_MG_L_ML_ANON.jpg ‚Üí 22580098_6200187f3f1ccc18_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 22580576_2a5b932da4ce5ca1_MG_L_ML_ANON.jpg ‚Üí 22580576_2a5b932da4ce5ca1_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 22580706_fe7d005dcbbfb46d_MG_R_ML_ANON.jpg ‚Üí 22580706_fe7d005dcbbfb46d_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 22613702_dcafa6ba6374ec07_MG_L_ML_ANON.jpg ‚Üí 22613702_dcafa6ba6374ec07_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 22613822_45c7f44839fd9e68_MG_R_ML_ANON.jpg ‚Üí 22613822_45c7f44839fd9e68_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 22614127_6bd24a0a42c19ce1_MG_R_ML_ANON.jpg ‚Üí 22614127_6bd24a0a42c19ce1_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 22614266_1e5c3af078f74b05_MG_L_ML_ANON.jpg ‚Üí 22614266_1e5c3af078f74b05_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 22614431_d065adcb9905b973_MG_L_ML_ANON.jpg ‚Üí 22614431_d065adcb9905b973_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 22670147_e1f51192f7bf3f5f_MG_R_ML_ANON.jpg ‚Üí 22670147_e1f51192f7bf3f5f_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 22670324_98429c0bdf78c0c7_MG_R_ML_ANON.jpg ‚Üí 22670324_98429c0bdf78c0c7_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 22670511_7e677f3d530e41ed_MG_L_ML_ANON.jpg ‚Üí 22670511_7e677f3d530e41ed_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 22670855_0b7396cdccacca82_MG_R_ML_ANON.jpg ‚Üí 22670855_0b7396cdccacca82_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 24055149_606e9b184978a350_MG_L_ML_ANON.jpg ‚Üí 24055149_606e9b184978a350_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 24055274_1e10aef17c9fe149_MG_L_ML_ANON.jpg ‚Üí 24055274_1e10aef17c9fe149_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 24055445_ac3185e18ffdc7b6_MG_L_ML_ANON.jpg ‚Üí 24055445_ac3185e18ffdc7b6_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 24055464_ac3185e18ffdc7b6_MG_R_ML_ANON.jpg ‚Üí 24055464_ac3185e18ffdc7b6_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 24055600_6f1aef40b3775182_MG_R_ML_ANON.jpg ‚Üí 24055600_6f1aef40b3775182_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 24065251_c4b995eddb3c510c_MG_L_ML_ANON.jpg ‚Üí 24065251_c4b995eddb3c510c_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 24065407_83db89f57aea498a_MG_R_ML_ANON.jpg ‚Üí 24065407_83db89f57aea498a_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 24065530_d8205a09c8173f44_MG_L_ML_ANON.jpg ‚Üí 24065530_d8205a09c8173f44_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 24065707_5291e1aee2bbf5df_MG_R_ML_ANON.jpg ‚Üí 24065707_5291e1aee2bbf5df_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 27829188_fbb55bf7fff48540_MG_R_ML_ANON.jpg ‚Üí 27829188_fbb55bf7fff48540_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 30011553_349323117bf0fd93_MG_L_ML_ANON.jpg ‚Üí 30011553_349323117bf0fd93_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 30011727_6968748e66837bc7_MG_L_ML_ANON.jpg ‚Üí 30011727_6968748e66837bc7_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50994841_069212ec65a94339_MG_R_ML_ANON.jpg ‚Üí 50994841_069212ec65a94339_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50996352_6aba0b402889a16f_MG_R_ML_ANON.jpg ‚Üí 50996352_6aba0b402889a16f_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50998059_66adfbb4f19c76d2_MG_L_ML_ANON.jpg ‚Üí 50998059_66adfbb4f19c76d2_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50998981_a78eba834ef6ee88_MG_R_ML_ANON.jpg ‚Üí 50998981_a78eba834ef6ee88_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 50999432_f62fbf38fb208316_MG_L_ML_ANON.jpg ‚Üí 50999432_f62fbf38fb208316_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 53580804_51bec6477a7898b9_MG_L_ML_ANON.jpg ‚Üí 53580804_51bec6477a7898b9_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 53581406_b231a8ba4dd4214f_MG_R_ML_ANON.jpg ‚Üí 53581406_b231a8ba4dd4214f_MG_R_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 53582656_465aa5ec1b59efc6_MG_L_ML_ANON.jpg ‚Üí 53582656_465aa5ec1b59efc6_MG_L_ML_ANON.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb023.jpg ‚Üí mdb023.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb028.jpg ‚Üí mdb028.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb058.jpg ‚Üí mdb058.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb072.jpg ‚Üí mdb072.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb075.jpg ‚Üí mdb075.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb090.jpg ‚Üí mdb090.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb092.jpg ‚Üí mdb092.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb095.jpg ‚Üí mdb095.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb102.jpg ‚Üí mdb102.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb105.jpg ‚Üí mdb105.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb110.jpg ‚Üí mdb110.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb111.jpg ‚Üí mdb111.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb115.jpg ‚Üí mdb115.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb117.jpg ‚Üí mdb117.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb120.jpg ‚Üí mdb120.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb124.jpg ‚Üí mdb124.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb125.jpg ‚Üí mdb125.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb130.jpg ‚Üí mdb130.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb134.jpg ‚Üí mdb134.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb141.jpg ‚Üí mdb141.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb144.jpg ‚Üí mdb144.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb148.jpg ‚Üí mdb148.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb155.jpg ‚Üí mdb155.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb158.jpg ‚Üí mdb158.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb170.jpg ‚Üí mdb170.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb171.jpg ‚Üí mdb171.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb178.jpg ‚Üí mdb178.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb179.jpg ‚Üí mdb179.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb181.jpg ‚Üí mdb181.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb184.jpg ‚Üí mdb184.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb186.jpg ‚Üí mdb186.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb202.jpg ‚Üí mdb202.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb206.jpg ‚Üí mdb206.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb209.jpg ‚Üí mdb209.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb220.jpg ‚Üí mdb220.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb221.jpg ‚Üí mdb221.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb222.jpg ‚Üí mdb222.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb223.jpg ‚Üí mdb223.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb224.jpg ‚Üí mdb224.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb225.jpg ‚Üí mdb225.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb226.jpg ‚Üí mdb226.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb227.jpg ‚Üí mdb227.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb228.jpg ‚Üí mdb228.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb229.jpg ‚Üí mdb229.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb230.jpg ‚Üí mdb230.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb232.jpg ‚Üí mdb232.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb234.jpg ‚Üí mdb234.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb235.jpg ‚Üí mdb235.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb236.jpg ‚Üí mdb236.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb237.jpg ‚Üí mdb237.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb240.jpg ‚Üí mdb240.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb242.jpg ‚Üí mdb242.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb243.jpg ‚Üí mdb243.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb244.jpg ‚Üí mdb244.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb246.jpg ‚Üí mdb246.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb247.jpg ‚Üí mdb247.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb248.jpg ‚Üí mdb248.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb250.jpg ‚Üí mdb250.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb251.jpg ‚Üí mdb251.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb252.jpg ‚Üí mdb252.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb254.jpg ‚Üí mdb254.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb255.jpg ‚Üí mdb255.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb257.jpg ‚Üí mdb257.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb258.jpg ‚Üí mdb258.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb259.jpg ‚Üí mdb259.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb260.jpg ‚Üí mdb260.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb261.jpg ‚Üí mdb261.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb262.jpg ‚Üí mdb262.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb263.jpg ‚Üí mdb263.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb266.jpg ‚Üí mdb266.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb268.jpg ‚Üí mdb268.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb269.jpg ‚Üí mdb269.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb272.jpg ‚Üí mdb272.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb273.jpg ‚Üí mdb273.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb275.jpg ‚Üí mdb275.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb276.jpg ‚Üí mdb276.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb277.jpg ‚Üí mdb277.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb278.jpg ‚Üí mdb278.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb279.jpg ‚Üí mdb279.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb280.jpg ‚Üí mdb280.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb281.jpg ‚Üí mdb281.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb282.jpg ‚Üí mdb282.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb283.jpg ‚Üí mdb283.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb284.jpg ‚Üí mdb284.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb285.jpg ‚Üí mdb285.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb286.jpg ‚Üí mdb286.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb287.jpg ‚Üí mdb287.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb288.jpg ‚Üí mdb288.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb289.jpg ‚Üí mdb289.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb290.jpg ‚Üí mdb290.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb291.jpg ‚Üí mdb291.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb292.jpg ‚Üí mdb292.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb293.jpg ‚Üí mdb293.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb294.jpg ‚Üí mdb294.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb295.jpg ‚Üí mdb295.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb296.jpg ‚Üí mdb296.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb297.jpg ‚Üí mdb297.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb298.jpg ‚Üí mdb298.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb299.jpg ‚Üí mdb299.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb300.jpg ‚Üí mdb300.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb301.jpg ‚Üí mdb301.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb302.jpg ‚Üí mdb302.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb303.jpg ‚Üí mdb303.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb304.jpg ‚Üí mdb304.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb305.jpg ‚Üí mdb305.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb306.jpg ‚Üí mdb306.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb307.jpg ‚Üí mdb307.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb308.jpg ‚Üí mdb308.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb309.jpg ‚Üí mdb309.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb310.jpg ‚Üí mdb310.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb311.jpg ‚Üí mdb311.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb312.jpg ‚Üí mdb312.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb313.jpg ‚Üí mdb313.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb314.jpg ‚Üí mdb314.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb315.jpg ‚Üí mdb315.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb316.jpg ‚Üí mdb316.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb317.jpg ‚Üí mdb317.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb318.jpg ‚Üí mdb318.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb319.jpg ‚Üí mdb319.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb320.jpg ‚Üí mdb320.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb321.jpg ‚Üí mdb321.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb322.jpg ‚Üí mdb322.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2018_BC0023145_ MLO_L.jpg ‚Üí 2018_BC0023145_ MLO_L.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2018_BC0023145_ MLO_R.jpg ‚Üí 2018_BC0023145_ MLO_R.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2018_BC0023241_ MLO_L.jpg ‚Üí 2018_BC0023241_ MLO_L.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2018_BC0023241_ MLO_R.jpg ‚Üí 2018_BC0023241_ MLO_R.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2018_BC0024341_ MLO_L.jpg ‚Üí 2018_BC0024341_ MLO_L.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2018_BC0024341_ MLO_R.jpg ‚Üí 2018_BC0024341_ MLO_R.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2019_BC0021728_ MLO_L.jpg ‚Üí 2019_BC0021728_ MLO_L.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2019_BC0021728_ MLO_R.jpg ‚Üí 2019_BC0021728_ MLO_R.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2019_BC0024581_ MLO_L.jpg ‚Üí 2019_BC0024581_ MLO_L.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2019_BC0024581_ MLO_R.jpg ‚Üí 2019_BC0024581_ MLO_R.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2019_BC0024763_ MLO_L.jpg ‚Üí 2019_BC0024763_ MLO_L.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2019_BC0025041_ MLO_L.jpg ‚Üí 2019_BC0025041_ MLO_L.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: 2019_BC0025041_ MLO_R.jpg ‚Üí 2019_BC0025041_ MLO_R.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb211.jpg ‚Üí mdb211.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb213.jpg ‚Üí mdb213.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb216.jpg ‚Üí mdb216.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb231.jpg ‚Üí mdb231.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb233.jpg ‚Üí mdb233.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb238.jpg ‚Üí mdb238.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb239.jpg ‚Üí mdb239.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb241.jpg ‚Üí mdb241.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb245.jpg ‚Üí mdb245.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb249.jpg ‚Üí mdb249.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb253.jpg ‚Üí mdb253.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb256.jpg ‚Üí mdb256.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb264.jpg ‚Üí mdb264.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb265.jpg ‚Üí mdb265.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb267.jpg ‚Üí mdb267.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb270.jpg ‚Üí mdb270.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb271.jpg ‚Üí mdb271.jpg\n",
      "‚úÖ Convertit »ôi redimensionat: mdb274.jpg ‚Üí mdb274.jpg\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def convert_and_resize_all_images(folder_path, delete_original=False, target_size=(224, 224)):\n",
    "    supported = [\".png\", \".pgm\", \".jpeg\", \".jpg\"]\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            ext = os.path.splitext(file)[1].lower()\n",
    "            if ext in supported:\n",
    "                full_path = os.path.join(root, file)\n",
    "                new_path = os.path.splitext(full_path)[0] + \".jpg\"\n",
    "                try:\n",
    "                    img = Image.open(full_path).convert(\"RGB\").resize(target_size)\n",
    "                    img.save(new_path, \"JPEG\", quality=95)\n",
    "                    print(f\"‚úÖ Convertit »ôi redimensionat: {file} ‚Üí {os.path.basename(new_path)}\")\n",
    "                    if delete_original and full_path != new_path:\n",
    "                        os.remove(full_path)\n",
    "                        print(f\"üóëÔ∏è »òters: {file}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Eroare la {file}: {e}\")\n",
    "\n",
    "convert_and_resize_all_images(\"model/\")\n",
    "convert_and_resize_all_images(\"validare/\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-24T14:43:11.882560200Z",
     "start_time": "2025-05-24T14:43:09.998339100Z"
    }
   },
   "id": "d5a3dca8cc7a0031",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [04:38<00:00, 10.32s/it, Train_Loss=0.0335, Train_Acc=78.30%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 1: Train Loss = 0.0335, Accuracy = 78.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\AI\\Incercare proiect AI\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\AI\\Incercare proiect AI\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\AI\\Incercare proiect AI\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Validation F1-score: 0.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.74      1.00      0.85        88\n",
      "      Malign       0.00      0.00      0.00        31\n",
      "\n",
      "    accuracy                           0.74       119\n",
      "   macro avg       0.37      0.50      0.43       119\n",
      "weighted avg       0.55      0.74      0.63       119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:  15%|‚ñà‚ñç        | 4/27 [01:11<06:52, 17.95s/it, Train_Loss=0.0290, Train_Acc=81.25%]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[17]\u001B[39m\u001B[32m, line 85\u001B[39m\n\u001B[32m     82\u001B[39m loss = loss_fn(outputs.logits, labels)\n\u001B[32m     84\u001B[39m optimizer.zero_grad()\n\u001B[32m---> \u001B[39m\u001B[32m85\u001B[39m \u001B[43mloss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     86\u001B[39m optimizer.step()\n\u001B[32m     88\u001B[39m running_loss += loss.item()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\AI\\Incercare proiect AI\\.venv\\Lib\\site-packages\\torch\\_tensor.py:648\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    638\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    639\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    640\u001B[39m         Tensor.backward,\n\u001B[32m    641\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    646\u001B[39m         inputs=inputs,\n\u001B[32m    647\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m648\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mautograd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    649\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\n\u001B[32m    650\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\AI\\Incercare proiect AI\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    348\u001B[39m     retain_graph = create_graph\n\u001B[32m    350\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    351\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    352\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m353\u001B[39m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    354\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    355\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    356\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    357\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    358\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    359\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    360\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    361\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\AI\\Incercare proiect AI\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    822\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    823\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m824\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_execution_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[32m    825\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    826\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    827\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    828\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim import AdamW\n",
    "from transformers import ViTForImageClassification\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============================\n",
    "# üîß CONFIG\n",
    "# ============================\n",
    "train_dir = \"model\"\n",
    "val_dir = \"validare\"\n",
    "batch_size = 16\n",
    "num_epochs = 10\n",
    "early_stop_patience = 3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ============================\n",
    "# üì¶ TRANSFORMƒÇRI\n",
    "# ============================\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "# ============================\n",
    "# üì• √éNCƒÇRCARE DATE\n",
    "# ============================\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=transform_train)\n",
    "val_dataset = datasets.ImageFolder(val_dir, transform=transform_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# ============================\n",
    "# üß† MODEL\n",
    "# ============================\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224-in21k\",\n",
    "    num_labels=2  # benign / malign\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# ============================\n",
    "# ‚öñÔ∏è LOSS + OPTIMIZER\n",
    "# ============================\n",
    "\n",
    "class_weights = torch.tensor([1.0, 1.0], dtype=torch.float).to(device) \n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# ============================\n",
    "# üö¶ TRAIN + VALIDARE + EARLY STOPPING\n",
    "# ============================\n",
    "best_f1 = 0\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for images, labels in progress_bar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(pixel_values=images)\n",
    "        loss = loss_fn(outputs.logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.logits, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        progress_bar.set_postfix({\n",
    "            \"Train_Loss\": f\"{running_loss / total:.4f}\",\n",
    "            \"Train_Acc\": f\"{(correct / total) * 100:.2f}%\"\n",
    "        })\n",
    "\n",
    "    print(f\"‚úÖ Epoch {epoch+1}: Train Loss = {running_loss / total:.4f}, Accuracy = {(correct / total) * 100:.2f}%\")\n",
    "\n",
    "    # VALIDARE\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(pixel_values=images)\n",
    "            _, preds = torch.max(outputs.logits, 1)\n",
    "\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    print(f\"üß™ Validation F1-score: {f1:.4f}\")\n",
    "    print(classification_report(y_true, y_pred, target_names=[\"Benign\", \"Malign\"]))\n",
    "\n",
    "    # EARLY STOPPING\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        epochs_no_improve = 0\n",
    "        output_dir = \"vit_mamografie_best_model_24mai\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        model.save_pretrained(output_dir)\n",
    "        print(f\"üíæ Model salvat (F1 = {f1:.4f}) √Æn: {output_dir}\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f\"‚èπÔ∏è Early stopping activat. F1 maxim: {best_f1:.4f}\")\n",
    "            break\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-24T14:30:12.224701300Z",
     "start_time": "2025-05-24T14:23:47.120193700Z"
    }
   },
   "id": "62c2fab2e098cf94",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [05:30<00:00, 12.23s/it, Train_Loss=0.0441, Train_Acc=71.46%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 1: Train Loss = 0.0441, Accuracy = 71.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\AI\\Incercare proiect AI\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\AI\\Incercare proiect AI\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\AI\\Incercare proiect AI\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Validation F1-score: 0.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.74      1.00      0.85        88\n",
      "      Malign       0.00      0.00      0.00        31\n",
      "\n",
      "    accuracy                           0.74       119\n",
      "   macro avg       0.37      0.50      0.43       119\n",
      "weighted avg       0.55      0.74      0.63       119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:   0%|          | 0/27 [00:15<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[18]\u001B[39m\u001B[32m, line 85\u001B[39m\n\u001B[32m     82\u001B[39m loss = loss_fn(outputs.logits, labels)\n\u001B[32m     84\u001B[39m optimizer.zero_grad()\n\u001B[32m---> \u001B[39m\u001B[32m85\u001B[39m \u001B[43mloss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     86\u001B[39m optimizer.step()\n\u001B[32m     88\u001B[39m running_loss += loss.item()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\AI\\Incercare proiect AI\\.venv\\Lib\\site-packages\\torch\\_tensor.py:648\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    638\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    639\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    640\u001B[39m         Tensor.backward,\n\u001B[32m    641\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    646\u001B[39m         inputs=inputs,\n\u001B[32m    647\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m648\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mautograd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    649\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\n\u001B[32m    650\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\AI\\Incercare proiect AI\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    348\u001B[39m     retain_graph = create_graph\n\u001B[32m    350\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    351\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    352\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m353\u001B[39m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    354\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    355\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    356\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    357\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    358\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    359\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    360\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    361\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\AI\\Incercare proiect AI\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    822\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    823\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m824\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_execution_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[32m    825\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    826\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    827\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    828\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim import AdamW\n",
    "from transformers import ViTForImageClassification\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============================\n",
    "# üîß CONFIG\n",
    "# ============================\n",
    "train_dir = \"model\"\n",
    "val_dir = \"validare\"\n",
    "batch_size = 16\n",
    "num_epochs = 10\n",
    "early_stop_patience = 3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ============================\n",
    "# üì¶ TRANSFORMƒÇRI\n",
    "# ============================\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "# ============================\n",
    "# üì• √éNCƒÇRCARE DATE\n",
    "# ============================\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=transform_train)\n",
    "val_dataset = datasets.ImageFolder(val_dir, transform=transform_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# ============================\n",
    "# üß† MODEL\n",
    "# ============================\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224-in21k\",\n",
    "    num_labels=2  # benign / malign\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# ============================\n",
    "# ‚öñÔ∏è LOSS + OPTIMIZER\n",
    "# ============================\n",
    "# Pondere pentru clasa minoritarƒÉ (malign = 40)\n",
    "class_weights = torch.tensor([1.0, 166 / 40], dtype=torch.float).to(device)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# ============================\n",
    "# üö¶ TRAIN + VALIDARE + EARLY STOPPING\n",
    "# ============================\n",
    "best_f1 = 0\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for images, labels in progress_bar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(pixel_values=images)\n",
    "        loss = loss_fn(outputs.logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.logits, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        progress_bar.set_postfix({\n",
    "            \"Train_Loss\": f\"{running_loss / total:.4f}\",\n",
    "            \"Train_Acc\": f\"{(correct / total) * 100:.2f}%\"\n",
    "        })\n",
    "\n",
    "    print(f\"‚úÖ Epoch {epoch+1}: Train Loss = {running_loss / total:.4f}, Accuracy = {(correct / total) * 100:.2f}%\")\n",
    "\n",
    "    # VALIDARE\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(pixel_values=images)\n",
    "            _, preds = torch.max(outputs.logits, 1)\n",
    "\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    print(f\"üß™ Validation F1-score: {f1:.4f}\")\n",
    "    print(classification_report(y_true, y_pred, target_names=[\"Benign\", \"Malign\"]))\n",
    "\n",
    "    # EARLY STOPPING\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        epochs_no_improve = 0\n",
    "        output_dir = \"vit_mamografie_24mai\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        model.save_pretrained(output_dir)\n",
    "        print(f\"üíæ Model salvat (F1 = {f1:.4f}) √Æn: {output_dir}\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f\"‚èπÔ∏è Early stopping activat. F1 maxim: {best_f1:.4f}\")\n",
    "            break\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-24T14:39:16.871414200Z",
     "start_time": "2025-05-24T14:32:54.249187300Z"
    }
   },
   "id": "ddc1ef9cf0cfb275",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribu»õie clase train: [348, 123]\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to C:\\Users\\magui/.cache\\torch\\hub\\checkpoints\\resnet50-0676ba61.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97.8M/97.8M [00:14<00:00, 7.10MB/s]\n",
      "Epoch 1/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [01:38<00:00,  3.29s/it, Loss=0.0318, Acc=60.30%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 1: Loss = 0.0318, Acc = 60.30%\n",
      "üß™ F1-score: 0.5000\n",
      "üéØ Etichete reale: Counter({np.int64(0): 88, np.int64(1): 31})\n",
      "üìä Predic»õii: Counter({np.int64(0): 94, np.int64(1): 25})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.82      0.88      0.85        88\n",
      "      Malign       0.56      0.45      0.50        31\n",
      "\n",
      "    accuracy                           0.76       119\n",
      "   macro avg       0.69      0.66      0.67       119\n",
      "weighted avg       0.75      0.76      0.76       119\n",
      "\n",
      "üíæ Model salvat √Æn: resnet50_mamografie_best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [03:05<00:00,  6.17s/it, Loss=0.0307, Acc=60.93%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 2: Loss = 0.0307, Acc = 60.93%\n",
      "üß™ F1-score: 0.4301\n",
      "üéØ Etichete reale: Counter({np.int64(0): 88, np.int64(1): 31})\n",
      "üìä Predic»õii: Counter({np.int64(1): 62, np.int64(0): 57})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.81      0.52      0.63        88\n",
      "      Malign       0.32      0.65      0.43        31\n",
      "\n",
      "    accuracy                           0.55       119\n",
      "   macro avg       0.56      0.58      0.53       119\n",
      "weighted avg       0.68      0.55      0.58       119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [02:31<00:00,  5.04s/it, Loss=0.0261, Acc=72.82%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 3: Loss = 0.0261, Acc = 72.82%\n",
      "üß™ F1-score: 0.4113\n",
      "üéØ Etichete reale: Counter({np.int64(0): 88, np.int64(1): 31})\n",
      "üìä Predic»õii: Counter({np.int64(1): 110, np.int64(0): 9})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.78      0.08      0.14        88\n",
      "      Malign       0.26      0.94      0.41        31\n",
      "\n",
      "    accuracy                           0.30       119\n",
      "   macro avg       0.52      0.51      0.28       119\n",
      "weighted avg       0.64      0.30      0.21       119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [03:55<00:00,  7.86s/it, Loss=0.0216, Acc=78.34%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 4: Loss = 0.0216, Acc = 78.34%\n",
      "üß™ F1-score: 0.5957\n",
      "üéØ Etichete reale: Counter({np.int64(0): 88, np.int64(1): 31})\n",
      "üìä Predic»õii: Counter({np.int64(0): 103, np.int64(1): 16})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.83      0.98      0.90        88\n",
      "      Malign       0.88      0.45      0.60        31\n",
      "\n",
      "    accuracy                           0.84       119\n",
      "   macro avg       0.85      0.71      0.75       119\n",
      "weighted avg       0.85      0.84      0.82       119\n",
      "\n",
      "üíæ Model salvat √Æn: resnet50_mamografie_best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [03:10<00:00,  6.37s/it, Loss=0.0249, Acc=76.22%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 5: Loss = 0.0249, Acc = 76.22%\n",
      "üß™ F1-score: 0.5532\n",
      "üéØ Etichete reale: Counter({np.int64(0): 88, np.int64(1): 31})\n",
      "üìä Predic»õii: Counter({np.int64(0): 103, np.int64(1): 16})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.83      0.97      0.89        88\n",
      "      Malign       0.81      0.42      0.55        31\n",
      "\n",
      "    accuracy                           0.82       119\n",
      "   macro avg       0.82      0.69      0.72       119\n",
      "weighted avg       0.82      0.82      0.80       119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [03:39<00:00,  7.33s/it, Loss=0.0179, Acc=83.23%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 6: Loss = 0.0179, Acc = 83.23%\n",
      "üß™ F1-score: 0.5246\n",
      "üéØ Etichete reale: Counter({np.int64(0): 88, np.int64(1): 31})\n",
      "üìä Predic»õii: Counter({np.int64(0): 89, np.int64(1): 30})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.83      0.84      0.84        88\n",
      "      Malign       0.53      0.52      0.52        31\n",
      "\n",
      "    accuracy                           0.76       119\n",
      "   macro avg       0.68      0.68      0.68       119\n",
      "weighted avg       0.75      0.76      0.75       119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [03:24<00:00,  6.81s/it, Loss=0.0127, Acc=89.60%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 7: Loss = 0.0127, Acc = 89.60%\n",
      "üß™ F1-score: 0.4384\n",
      "üéØ Etichete reale: Counter({np.int64(0): 88, np.int64(1): 31})\n",
      "üìä Predic»õii: Counter({np.int64(0): 77, np.int64(1): 42})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.81      0.70      0.75        88\n",
      "      Malign       0.38      0.52      0.44        31\n",
      "\n",
      "    accuracy                           0.66       119\n",
      "   macro avg       0.59      0.61      0.59       119\n",
      "weighted avg       0.69      0.66      0.67       119\n",
      "\n",
      "‚èπÔ∏è Early stopping activat. F1 maxim: 0.5957\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "# ============================ üîß CONFIG ============================\n",
    "train_dir = \"model\"\n",
    "val_dir = \"validare\"\n",
    "batch_size = 16\n",
    "num_epochs = 10\n",
    "early_stop_patience = 3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ============================ üì¶ TRANSFORMƒÇRI ============================\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# ============================ üì• √éNCƒÇRCARE DATE ============================\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=transform_train)\n",
    "val_dataset = datasets.ImageFolder(val_dir, transform=transform_val)\n",
    "\n",
    "targets = train_dataset.targets\n",
    "class_counts = [targets.count(0), targets.count(1)]\n",
    "print(\"Distribu»õie clase train:\", class_counts)\n",
    "\n",
    "# Ponderi invers propor»õionale pentru sampler\n",
    "weights = [1.0 / class_counts[t] for t in targets]\n",
    "sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# ============================ üß† MODEL ============================\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# √énlocuim ultimul fully-connected layer\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 2)\n",
    "model.to(device)\n",
    "\n",
    "# ============================ ‚öñÔ∏è LOSS + OPTIM ============================\n",
    "# Pondere pentru clasa malign\n",
    "class_weights = torch.tensor([1.0, class_counts[0] / class_counts[1]], dtype=torch.float).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "# ============================ üö¶ TRAIN + VALIDARE ============================\n",
    "best_f1 = 0\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    for images, labels in progress_bar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        progress_bar.set_postfix({\n",
    "            \"Loss\": f\"{running_loss / total:.4f}\",\n",
    "            \"Acc\": f\"{(correct / total) * 100:.2f}%\"\n",
    "        })\n",
    "\n",
    "    print(f\"‚úÖ Epoch {epoch+1}: Loss = {running_loss / total:.4f}, Acc = {(correct / total) * 100:.2f}%\")\n",
    "\n",
    "    # VALIDARE\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    print(f\"üß™ F1-score: {f1:.4f}\")\n",
    "    print(\"üéØ Etichete reale:\", Counter(y_true))\n",
    "    print(\"üìä Predic»õii:\", Counter(y_pred))\n",
    "    print(classification_report(y_true, y_pred, target_names=[\"Benign\", \"Malign\"]))\n",
    "\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        epochs_no_improve = 0\n",
    "        output_dir = \"resnet50_mamografie_best\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        torch.save(model.state_dict(), os.path.join(output_dir, \"model.pt\"))\n",
    "        print(f\"üíæ Model salvat √Æn: {output_dir}\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f\"‚èπÔ∏è Early stopping activat. F1 maxim: {best_f1:.4f}\")\n",
    "            break\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-24T15:07:36.796039800Z",
     "start_time": "2025-05-24T14:43:36.515211600Z"
    }
   },
   "id": "c30b7acdf9a71c3",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Distribu»õie clase train: [348, 123]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\AI\\Incercare proiect AI\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\AI\\Incercare proiect AI\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Epoch 1/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [01:46<00:00,  3.55s/it, Loss=0.0196, Acc=68.58%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 1: Train Loss = 0.0196\n",
      "üß™ Epoch 1 ‚Üí Precision: 0.4000 | Recall: 0.5161 | F1: 0.4507\n",
      "üéØ Etichete reale: Counter({np.int64(0): 88, np.int64(1): 31})\n",
      "üìä Predic»õii: Counter({np.int64(0): 79, np.int64(1): 40})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.81      0.73      0.77        88\n",
      "      Malign       0.40      0.52      0.45        31\n",
      "\n",
      "    accuracy                           0.67       119\n",
      "   macro avg       0.61      0.62      0.61       119\n",
      "weighted avg       0.70      0.67      0.68       119\n",
      "\n",
      "üíæ Model salvat √Æn: resnet50_focus_recall_best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [01:36<00:00,  3.21s/it, Loss=0.0151, Acc=79.83%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 2: Train Loss = 0.0151\n",
      "üß™ Epoch 2 ‚Üí Precision: 0.2632 | Recall: 0.9677 | F1: 0.4138\n",
      "üéØ Etichete reale: Counter({np.int64(0): 88, np.int64(1): 31})\n",
      "üìä Predic»õii: Counter({np.int64(1): 114, np.int64(0): 5})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.80      0.05      0.09        88\n",
      "      Malign       0.26      0.97      0.41        31\n",
      "\n",
      "    accuracy                           0.29       119\n",
      "   macro avg       0.53      0.51      0.25       119\n",
      "weighted avg       0.66      0.29      0.17       119\n",
      "\n",
      "üíæ Model salvat √Æn: resnet50_focus_recall_best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [02:57<00:00,  5.91s/it, Loss=0.0170, Acc=74.31%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 3: Train Loss = 0.0170\n",
      "üß™ Epoch 3 ‚Üí Precision: 0.4857 | Recall: 0.5484 | F1: 0.5152\n",
      "üéØ Etichete reale: Counter({np.int64(0): 88, np.int64(1): 31})\n",
      "üìä Predic»õii: Counter({np.int64(0): 84, np.int64(1): 35})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.83      0.80      0.81        88\n",
      "      Malign       0.49      0.55      0.52        31\n",
      "\n",
      "    accuracy                           0.73       119\n",
      "   macro avg       0.66      0.67      0.66       119\n",
      "weighted avg       0.74      0.73      0.74       119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [01:32<00:00,  3.08s/it, Loss=0.0137, Acc=81.74%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 4: Train Loss = 0.0137\n",
      "üß™ Epoch 4 ‚Üí Precision: 0.2672 | Recall: 1.0000 | F1: 0.4218\n",
      "üéØ Etichete reale: Counter({np.int64(0): 88, np.int64(1): 31})\n",
      "üìä Predic»õii: Counter({np.int64(1): 116, np.int64(0): 3})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       1.00      0.03      0.07        88\n",
      "      Malign       0.27      1.00      0.42        31\n",
      "\n",
      "    accuracy                           0.29       119\n",
      "   macro avg       0.63      0.52      0.24       119\n",
      "weighted avg       0.81      0.29      0.16       119\n",
      "\n",
      "üíæ Model salvat √Æn: resnet50_focus_recall_best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [01:37<00:00,  3.26s/it, Loss=0.0148, Acc=80.47%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 5: Train Loss = 0.0148\n",
      "üß™ Epoch 5 ‚Üí Precision: 0.2768 | Recall: 1.0000 | F1: 0.4336\n",
      "üéØ Etichete reale: Counter({np.int64(0): 88, np.int64(1): 31})\n",
      "üìä Predic»õii: Counter({np.int64(1): 112, np.int64(0): 7})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       1.00      0.08      0.15        88\n",
      "      Malign       0.28      1.00      0.43        31\n",
      "\n",
      "    accuracy                           0.32       119\n",
      "   macro avg       0.64      0.54      0.29       119\n",
      "weighted avg       0.81      0.32      0.22       119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [01:40<00:00,  3.35s/it, Loss=0.0120, Acc=85.77%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 6: Train Loss = 0.0120\n",
      "üß™ Epoch 6 ‚Üí Precision: 0.2809 | Recall: 0.8065 | F1: 0.4167\n",
      "üéØ Etichete reale: Counter({np.int64(0): 88, np.int64(1): 31})\n",
      "üìä Predic»õii: Counter({np.int64(1): 89, np.int64(0): 30})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.80      0.27      0.41        88\n",
      "      Malign       0.28      0.81      0.42        31\n",
      "\n",
      "    accuracy                           0.41       119\n",
      "   macro avg       0.54      0.54      0.41       119\n",
      "weighted avg       0.66      0.41      0.41       119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [02:38<00:00,  5.27s/it, Loss=0.0106, Acc=85.35%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 7: Train Loss = 0.0106\n",
      "üß™ Epoch 7 ‚Üí Precision: 0.3485 | Recall: 0.7419 | F1: 0.4742\n",
      "üéØ Etichete reale: Counter({np.int64(0): 88, np.int64(1): 31})\n",
      "üìä Predic»õii: Counter({np.int64(1): 66, np.int64(0): 53})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.85      0.51      0.64        88\n",
      "      Malign       0.35      0.74      0.47        31\n",
      "\n",
      "    accuracy                           0.57       119\n",
      "   macro avg       0.60      0.63      0.56       119\n",
      "weighted avg       0.72      0.57      0.60       119\n",
      "\n",
      "‚èπÔ∏è Early stopping activat. Recall maxim: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import csv\n",
    "\n",
    "# ---------------------CONFIG-----------------------\n",
    "train_dir = \"model\"\n",
    "val_dir = \"validare\"\n",
    "batch_size = 16\n",
    "num_epochs = 10\n",
    "early_stop_patience = 3\n",
    "threshold = 0.3  #prag\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#-------------------TRANSFORMARI----------------------------\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "#------------------------DATE----------------------------------\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=transform_train)\n",
    "val_dataset = datasets.ImageFolder(val_dir, transform=transform_val)\n",
    "\n",
    "targets = train_dataset.targets\n",
    "class_counts = [targets.count(0), targets.count(1)]\n",
    "print(\"Distributie clase train:\", class_counts)\n",
    "\n",
    "weights = [1.0 / class_counts[t] for t in targets]\n",
    "sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "#------------------------MODEL--------------------------------\n",
    "model = models.resnet50(pretrained=True)\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 2)\n",
    "model.to(device)\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=2.0, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = nn.functional.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        loss = self.alpha * ((1 - pt) ** self.gamma) * ce_loss\n",
    "        return loss.mean()\n",
    "\n",
    "loss_fn = FocalLoss(alpha=2.0, gamma=2.0)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "#---------------------------TRAIN-------------------------\n",
    "best_recall = 0\n",
    "epochs_no_improve = 0\n",
    "\n",
    "with open(\"fisier_log.csv\", mode=\"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"Epoch\", \"Precision\", \"Recall\", \"F1\"])\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        for images, labels in progress_bar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            progress_bar.set_postfix({\n",
    "                \"Loss\": f\"{running_loss / total:.4f}\",\n",
    "                \"Acc\": f\"{(correct / total) * 100:.2f}%\"\n",
    "            })\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {running_loss / total:.4f}\")\n",
    "\n",
    "        #---------------------VALIDARE--------------------------\n",
    "        model.eval()\n",
    "        y_true, y_pred = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "\n",
    "                probs = torch.softmax(outputs, dim=1)[:, 1]  # probabilitate malign\n",
    "                preds = (probs > threshold).long()\n",
    "\n",
    "                y_true.extend(labels.cpu().numpy())\n",
    "                y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} ‚Üí Precision: {precision:.4f} | Recall: {recall:.4f} | F1: {f1:.4f}\")\n",
    "        print(\"Etichete reale:\", Counter(y_true))\n",
    "        print(\"Predictii:\", Counter(y_pred))\n",
    "        print(classification_report(y_true, y_pred, target_names=[\"Benign\", \"Malign\"]))\n",
    "\n",
    "        writer.writerow([epoch+1, precision, recall, f1])\n",
    "\n",
    "        # Early stopping\n",
    "        if recall > best_recall:\n",
    "            best_recall = recall\n",
    "            epochs_no_improve = 0\n",
    "            output_dir = \"resnet50_mamografie_model\"\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            torch.save(model.state_dict(), os.path.join(output_dir, \"model.pt\"))\n",
    "            print(f\"Model salvat in: {output_dir}\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= early_stop_patience:\n",
    "                print(f\"Early stopping activat. Recall maxim: {best_recall:.4f}\")\n",
    "                break\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-24T15:30:05.581033800Z",
     "start_time": "2025-05-24T15:14:46.221195900Z"
    }
   },
   "id": "e7444317901e0b8b",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1000x600 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def plot_metrice_din_csv(csv_path=\"epoca_log.csv\"):\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(\"CSV-ul nu exista.\")\n",
    "        return\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(df[\"Epoch\"], df[\"Precision\"], label=\"Precision\", marker=\"o\")\n",
    "    plt.plot(df[\"Epoch\"], df[\"Recall\"], label=\"Recall\", marker=\"o\")\n",
    "    plt.plot(df[\"Epoch\"], df[\"F1\"], label=\"F1-score\", marker=\"o\")\n",
    "\n",
    "    plt.title(\"Evolutia metricilor pe epoci\")\n",
    "    plt.xlabel(\"EpocƒÉ\")\n",
    "    plt.ylabel(\"Valoare metrica\")\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"evolutie_metrici.png\")\n",
    "    plt.show()\n",
    "\n",
    "plot_metrice_din_csv()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-24T15:31:09.714747300Z",
     "start_time": "2025-05-24T15:31:08.695829100Z"
    }
   },
   "id": "6f985949f751c1e6",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distributie clase train: [348, 123]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\AI\\Incercare proiect AI\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\AI\\Incercare proiect AI\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Epoch 1/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [01:36<00:00,  3.23s/it, Loss=0.0179, Acc=74.73%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.0179\n",
      "Epoch 1 ‚Üí Precision: 0.2605 | Recall: 1.0000 | F1: 0.4133\n",
      "Etichete reale: Counter({np.int64(0): 88, np.int64(1): 31})\n",
      "Predictii: Counter({np.int64(1): 119})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.00      0.00      0.00        88\n",
      "      Malign       0.26      1.00      0.41        31\n",
      "\n",
      "    accuracy                           0.26       119\n",
      "   macro avg       0.13      0.50      0.21       119\n",
      "weighted avg       0.07      0.26      0.11       119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\AI\\Incercare proiect AI\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\AI\\Incercare proiect AI\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\AI\\Incercare proiect AI\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model salvat in: resnet50_mamografie_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [01:43<00:00,  3.46s/it, Loss=0.0172, Acc=74.95%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss = 0.0172\n",
      "Epoch 2 ‚Üí Precision: 0.2703 | Recall: 0.9677 | F1: 0.4225\n",
      "Etichete reale: Counter({np.int64(0): 88, np.int64(1): 31})\n",
      "Predictii: Counter({np.int64(1): 111, np.int64(0): 8})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.88      0.08      0.15        88\n",
      "      Malign       0.27      0.97      0.42        31\n",
      "\n",
      "    accuracy                           0.31       119\n",
      "   macro avg       0.57      0.52      0.28       119\n",
      "weighted avg       0.72      0.31      0.22       119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [01:39<00:00,  3.31s/it, Loss=0.0153, Acc=76.65%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss = 0.0153\n",
      "Epoch 3 ‚Üí Precision: 0.2718 | Recall: 0.9032 | F1: 0.4179\n",
      "Etichete reale: Counter({np.int64(0): 88, np.int64(1): 31})\n",
      "Predictii: Counter({np.int64(1): 103, np.int64(0): 16})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.81      0.15      0.25        88\n",
      "      Malign       0.27      0.90      0.42        31\n",
      "\n",
      "    accuracy                           0.34       119\n",
      "   macro avg       0.54      0.53      0.33       119\n",
      "weighted avg       0.67      0.34      0.29       119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 28/30 [01:41<00:07,  3.61s/it, Loss=0.0149, Acc=81.92%]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 94\u001B[39m\n\u001B[32m     91\u001B[39m loss = loss_fn(outputs, labels)\n\u001B[32m     93\u001B[39m optimizer.zero_grad()\n\u001B[32m---> \u001B[39m\u001B[32m94\u001B[39m \u001B[43mloss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     95\u001B[39m optimizer.step()\n\u001B[32m     97\u001B[39m running_loss += loss.item()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\AI\\Incercare proiect AI\\.venv\\Lib\\site-packages\\torch\\_tensor.py:648\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    638\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    639\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    640\u001B[39m         Tensor.backward,\n\u001B[32m    641\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    646\u001B[39m         inputs=inputs,\n\u001B[32m    647\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m648\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mautograd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    649\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\n\u001B[32m    650\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\AI\\Incercare proiect AI\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    348\u001B[39m     retain_graph = create_graph\n\u001B[32m    350\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    351\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    352\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m353\u001B[39m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    354\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    355\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    356\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    357\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    358\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    359\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    360\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    361\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\AI\\Incercare proiect AI\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    822\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    823\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m824\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_execution_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[32m    825\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    826\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    827\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    828\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import csv\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# -------------------CONFIG-------------------------\n",
    "train_dir = \"model\"\n",
    "val_dir = \"validare\"\n",
    "batch_size = 16\n",
    "num_epochs = 10\n",
    "early_stop_patience = 3\n",
    "threshold = 0.3  #prag\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#-------------------TRANSFORMARI----------------------------\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "#---------------------DATE----------------------------------\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=transform_train)\n",
    "val_dataset = datasets.ImageFolder(val_dir, transform=transform_val)\n",
    "\n",
    "targets = train_dataset.targets\n",
    "class_counts = [targets.count(0), targets.count(1)]\n",
    "print(\"Distributie clase train:\", class_counts)\n",
    "\n",
    "weights = [1.0 / class_counts[t] for t in targets]\n",
    "sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "#------------------------MODEL--------------------------------\n",
    "model = models.resnet50(pretrained=True)\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 2)\n",
    "model.to(device)\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=2.0, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = nn.functional.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        loss = self.alpha * ((1 - pt) ** self.gamma) * ce_loss\n",
    "        return loss.mean()\n",
    "\n",
    "loss_fn = FocalLoss(alpha=2.0, gamma=2.0)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "#---------------------------TRAIN-------------------------\n",
    "best_recall = 0\n",
    "epochs_no_improve = 0\n",
    "\n",
    "with open(\"fisier_log.csv\", mode=\"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"Epoch\", \"Precision\", \"Recall\", \"F1\"])\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        for images, labels in progress_bar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            progress_bar.set_postfix({\n",
    "                \"Loss\": f\"{running_loss / total:.4f}\",\n",
    "                \"Acc\": f\"{(correct / total) * 100:.2f}%\"\n",
    "            })\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {running_loss / total:.4f}\")\n",
    "\n",
    "        #---------------------VALIDARE--------------------------\n",
    "        model.eval()\n",
    "        y_true, y_pred = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "\n",
    "                probs = torch.softmax(outputs, dim=1)[:, 1]  # probabilitate malign\n",
    "                preds = (probs > threshold).long()\n",
    "\n",
    "                y_true.extend(labels.cpu().numpy())\n",
    "                y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} ‚Üí Precision: {precision:.4f} | Recall: {recall:.4f} | F1: {f1:.4f}\")\n",
    "        print(\"Etichete reale:\", Counter(y_true))\n",
    "        print(\"Predictii:\", Counter(y_pred))\n",
    "        print(classification_report(y_true, y_pred, target_names=[\"Benign\", \"Malign\"]))\n",
    "\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        plt.figure(figsize=(5, 4))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Benign\", \"Malign\"], yticklabels=[\"Benign\", \"Malign\"])\n",
    "        plt.xlabel(\"Predictii\")\n",
    "        plt.ylabel(\"Etichete reale\")\n",
    "        plt.title(f\"Matricea de Confuzie - Epoca {epoch+1}\")\n",
    "        os.makedirs(\"confusion_matrices\", exist_ok=True)\n",
    "        plt.savefig(f\"confusion_matrices/conf_matrix_epoch_{epoch+1}.png\")\n",
    "        plt.close()\n",
    "        writer.writerow([epoch+1, precision, recall, f1])\n",
    "\n",
    "        # Early stopping\n",
    "        if recall > best_recall:\n",
    "            best_recall = recall\n",
    "            epochs_no_improve = 0\n",
    "            output_dir = \"resnet50_mamografie_model\"\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            torch.save(model.state_dict(), os.path.join(output_dir, \"model.pt\"))\n",
    "            print(f\"Model salvat in: {output_dir}\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= early_stop_patience:\n",
    "                print(f\"Early stopping activat. Recall maxim: {best_recall:.4f}\")\n",
    "                break\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-31T05:56:46.755320900Z",
     "start_time": "2025-05-31T05:49:11.740828900Z"
    }
   },
   "id": "c7d31f72bfc9090c",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribu»õie clase train: [348, 123]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [01:53<00:00,  3.79s/it, Loss=0.0316, Acc=59.24%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 1: Loss = 0.0316, Acc = 59.24%\n",
      "üß™ F1-score: 0.4598\n",
      "üéØ Etichete reale: Counter({np.int64(0): 88, np.int64(1): 31})\n",
      "üìä Predic»õii: Counter({np.int64(0): 63, np.int64(1): 56})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.83      0.59      0.69        88\n",
      "      Malign       0.36      0.65      0.46        31\n",
      "\n",
      "    accuracy                           0.61       119\n",
      "   macro avg       0.59      0.62      0.57       119\n",
      "weighted avg       0.70      0.61      0.63       119\n",
      "\n",
      "üíæ Model salvat √Æn: resnet50_mamografie_best_now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [01:35<00:00,  3.17s/it, Loss=0.0267, Acc=69.21%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 2: Loss = 0.0267, Acc = 69.21%\n",
      "üß™ F1-score: 0.4615\n",
      "üéØ Etichete reale: Counter({np.int64(0): 88, np.int64(1): 31})\n",
      "üìä Predic»õii: Counter({np.int64(1): 73, np.int64(0): 46})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.85      0.44      0.58        88\n",
      "      Malign       0.33      0.77      0.46        31\n",
      "\n",
      "    accuracy                           0.53       119\n",
      "   macro avg       0.59      0.61      0.52       119\n",
      "weighted avg       0.71      0.53      0.55       119\n",
      "\n",
      "üíæ Model salvat √Æn: resnet50_mamografie_best_now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [02:26<00:00,  4.87s/it, Loss=0.0214, Acc=79.41%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 3: Loss = 0.0214, Acc = 79.41%\n",
      "üß™ F1-score: 0.4317\n",
      "üéØ Etichete reale: Counter({np.int64(0): 88, np.int64(1): 31})\n",
      "üìä Predic»õii: Counter({np.int64(1): 108, np.int64(0): 11})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.91      0.11      0.20        88\n",
      "      Malign       0.28      0.97      0.43        31\n",
      "\n",
      "    accuracy                           0.34       119\n",
      "   macro avg       0.59      0.54      0.32       119\n",
      "weighted avg       0.74      0.34      0.26       119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [02:02<00:00,  4.07s/it, Loss=0.0216, Acc=81.10%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 4: Loss = 0.0216, Acc = 81.10%\n",
      "üß™ F1-score: 0.5660\n",
      "üéØ Etichete reale: Counter({np.int64(0): 88, np.int64(1): 31})\n",
      "üìä Predic»õii: Counter({np.int64(0): 97, np.int64(1): 22})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.84      0.92      0.88        88\n",
      "      Malign       0.68      0.48      0.57        31\n",
      "\n",
      "    accuracy                           0.81       119\n",
      "   macro avg       0.76      0.70      0.72       119\n",
      "weighted avg       0.80      0.81      0.80       119\n",
      "\n",
      "üíæ Model salvat √Æn: resnet50_mamografie_best_now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [01:16<00:00,  2.56s/it, Loss=0.0212, Acc=81.95%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 5: Loss = 0.0212, Acc = 81.95%\n",
      "üß™ F1-score: 0.5556\n",
      "üéØ Etichete reale: Counter({np.int64(0): 88, np.int64(1): 31})\n",
      "üìä Predic»õii: Counter({np.int64(0): 96, np.int64(1): 23})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.83      0.91      0.87        88\n",
      "      Malign       0.65      0.48      0.56        31\n",
      "\n",
      "    accuracy                           0.80       119\n",
      "   macro avg       0.74      0.70      0.71       119\n",
      "weighted avg       0.79      0.80      0.79       119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [01:47<00:00,  3.58s/it, Loss=0.0172, Acc=85.14%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 6: Loss = 0.0172, Acc = 85.14%\n",
      "üß™ F1-score: 0.4375\n",
      "üéØ Etichete reale: Counter({np.int64(0): 88, np.int64(1): 31})\n",
      "üìä Predic»õii: Counter({np.int64(1): 65, np.int64(0): 54})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.81      0.50      0.62        88\n",
      "      Malign       0.32      0.68      0.44        31\n",
      "\n",
      "    accuracy                           0.55       119\n",
      "   macro avg       0.57      0.59      0.53       119\n",
      "weighted avg       0.69      0.55      0.57       119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [02:41<00:00,  5.38s/it, Loss=0.0131, Acc=89.17%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 7: Loss = 0.0131, Acc = 89.17%\n",
      "üß™ F1-score: 0.5051\n",
      "üéØ Etichete reale: Counter({np.int64(0): 88, np.int64(1): 31})\n",
      "üìä Predic»õii: Counter({np.int64(1): 68, np.int64(0): 51})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.88      0.51      0.65        88\n",
      "      Malign       0.37      0.81      0.51        31\n",
      "\n",
      "    accuracy                           0.59       119\n",
      "   macro avg       0.62      0.66      0.58       119\n",
      "weighted avg       0.75      0.59      0.61       119\n",
      "\n",
      "‚èπÔ∏è Early stopping activat. F1 maxim: 0.5660\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "# ============================ üîß CONFIG ============================\n",
    "train_dir = \"model\"\n",
    "val_dir = \"validare\"\n",
    "batch_size = 16\n",
    "num_epochs = 10\n",
    "early_stop_patience = 3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ============================ üì¶ TRANSFORMƒÇRI ============================\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# ============================ üì• √éNCƒÇRCARE DATE ============================\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=transform_train)\n",
    "val_dataset = datasets.ImageFolder(val_dir, transform=transform_val)\n",
    "\n",
    "targets = train_dataset.targets\n",
    "class_counts = [targets.count(0), targets.count(1)]\n",
    "print(\"Distribu»õie clase train:\", class_counts)\n",
    "\n",
    "# Ponderi invers propor»õionale pentru sampler\n",
    "weights = [1.0 / class_counts[t] for t in targets]\n",
    "sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# ============================ üß† MODEL ============================\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# √énlocuim ultimul fully-connected layer\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 2)\n",
    "model.to(device)\n",
    "\n",
    "# ============================ ‚öñÔ∏è LOSS + OPTIM ============================\n",
    "# Pondere pentru clasa malign\n",
    "class_weights = torch.tensor([1.0, class_counts[0] / class_counts[1]], dtype=torch.float).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "# ============================ üö¶ TRAIN + VALIDARE ============================\n",
    "best_f1 = 0\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    for images, labels in progress_bar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        progress_bar.set_postfix({\n",
    "            \"Loss\": f\"{running_loss / total:.4f}\",\n",
    "            \"Acc\": f\"{(correct / total) * 100:.2f}%\"\n",
    "        })\n",
    "\n",
    "    print(f\"‚úÖ Epoch {epoch+1}: Loss = {running_loss / total:.4f}, Acc = {(correct / total) * 100:.2f}%\")\n",
    "\n",
    "    # VALIDARE\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    print(f\"üß™ F1-score: {f1:.4f}\")\n",
    "    print(\"üéØ Etichete reale:\", Counter(y_true))\n",
    "    print(\"üìä Predic»õii:\", Counter(y_pred))\n",
    "    print(classification_report(y_true, y_pred, target_names=[\"Benign\", \"Malign\"]))\n",
    "\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        epochs_no_improve = 0\n",
    "        output_dir = \"resnet50_mamografie_best_now\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        torch.save(model.state_dict(), os.path.join(output_dir, \"model.pt\"))\n",
    "        print(f\"üíæ Model salvat √Æn: {output_dir}\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f\"‚èπÔ∏è Early stopping activat. F1 maxim: {best_f1:.4f}\")\n",
    "            break\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-31T06:12:38.953708300Z",
     "start_time": "2025-05-31T05:57:22.099187300Z"
    }
   },
   "id": "2cf6def61a86103b",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d9376ff60a75e0fb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
